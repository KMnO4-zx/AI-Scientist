{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "import openai\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 backoff 库装饰器来处理 API 调用中的速率限制和超时异常，自动重试\n",
    "@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))\n",
    "def get_response_from_llm(\n",
    "    msg,  # 用户输入的消息\n",
    "    client,  # 用于与 API 交互的客户端对象\n",
    "    model,  # 指定使用的模型\n",
    "    system_message,  # 系统消息，用于设定对话上下文\n",
    "    print_debug=False,  # 是否打印调试信息\n",
    "    msg_history=None,  # 对话的历史记录\n",
    "    temperature=0.75,  # 生成的文本的多样性\n",
    "):\n",
    "    if msg_history is None:\n",
    "        msg_history = []  # 如果没有提供历史记录，则初始化为空列表\n",
    "\n",
    "    # 如果模型是 OpenAI 系列的 GPT-4o 模型之一\n",
    "    if model in [\n",
    "        \"gpt-4o-2024-05-13\",\n",
    "        \"gpt-4o-mini-2024-07-18\",\n",
    "        \"gpt-4o-2024-08-06\",\n",
    "        \"gpt-4o\",\n",
    "    ]:\n",
    "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]  # 将用户消息添加到历史记录中\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,  # 使用的模型名称\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},  # 系统消息\n",
    "                *new_msg_history,  # 历史消息记录\n",
    "            ],\n",
    "            temperature=temperature,  # 生成文本的多样性\n",
    "            max_tokens=3000,  # 最大生成的 token 数量\n",
    "            n=1,  # 请求生成一个响应\n",
    "            stop=None,  # 没有特定的停止条件\n",
    "            seed=0,  # 设置随机种子，确保生成的一致性\n",
    "        )\n",
    "        content = response.choices[0].message.content  # 从响应中提取生成的文本内容\n",
    "        new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]  # 更新历史记录\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model} not supported.\")\n",
    "\n",
    "\n",
    "    # 如果设置了打印调试信息\n",
    "    if print_debug:\n",
    "        print()\n",
    "        print(\"*\" * 20 + \" LLM START \" + \"*\" * 20)  # 打印调试分隔符\n",
    "        for j, msg in enumerate(new_msg_history):  # 遍历打印消息历史记录\n",
    "            print(f'{j}, {msg[\"role\"]}: {msg[\"content\"]}')\n",
    "        print(content)  # 打印生成的内容\n",
    "        print(\"*\" * 21 + \" LLM END \" + \"*\" * 21)  # 打印调试分隔符\n",
    "        print()\n",
    "\n",
    "    return content, new_msg_history  # 返回生成的内容和更新后的消息历史记录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APITimeoutError))\n",
    "def get_batch_responses_from_llm(\n",
    "    msg,  # 用户输入的消息\n",
    "    client,  # 用于与 API 交互的客户端对象\n",
    "    model,  # 指定使用的模型\n",
    "    system_message,  # 系统消息，用于设定对话上下文\n",
    "    print_debug=False,  # 是否打印调试信息\n",
    "    msg_history=None,  # 对话的历史记录\n",
    "    temperature=0.75,  # 生成的文本的多样性\n",
    "    n_responses=1,  # 需要生成的响应数量\n",
    "):\n",
    "    if msg_history is None:\n",
    "        msg_history = []  # 如果没有提供历史记录，则初始化为空列表\n",
    "\n",
    "    # 如果指定的模型是 OpenAI 系列的 GPT-4o 模型之一\n",
    "    if model in [\n",
    "        \"gpt-4o-2024-05-13\",\n",
    "        \"gpt-4o-mini-2024-07-18\",\n",
    "        \"gpt-4o-2024-08-06\",\n",
    "    ]:\n",
    "        new_msg_history = msg_history + [{\"role\": \"user\", \"content\": msg}]  # 将用户消息添加到历史记录中\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,  # 使用的模型名称\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},  # 系统消息\n",
    "                *new_msg_history,  # 历史消息记录\n",
    "            ],\n",
    "            temperature=temperature,  # 生成文本的多样性\n",
    "            max_tokens=3000,  # 最大生成的 token 数量\n",
    "            n=n_responses,  # 请求生成的响应数量\n",
    "            stop=None,  # 没有特定的停止条件\n",
    "            seed=0,  # 设置随机种子，确保生成的一致性\n",
    "        )\n",
    "        content = [r.message.content for r in response.choices]  # 从响应中提取生成的文本内容\n",
    "        new_msg_history = [\n",
    "            new_msg_history + [{\"role\": \"assistant\", \"content\": c}] for c in content  # 将每个响应加入新的历史记录\n",
    "        ]\n",
    "\n",
    "    # 如果模型不在支持的列表中\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model} not supported.\")  # 抛出异常，模型不支持\n",
    "\n",
    "    # 如果设置了打印调试信息\n",
    "    if print_debug:\n",
    "        print()\n",
    "        print(\"*\" * 20 + \" LLM START \" + \"*\" * 20)  # 打印调试分隔符\n",
    "        for j, msg in enumerate(new_msg_history[0]):  # 遍历打印第一条消息历史记录\n",
    "            print(f'{j}, {msg[\"role\"]}: {msg[\"content\"]}')\n",
    "        print(content)  # 打印生成的内容\n",
    "        print(\"*\" * 21 + \" LLM END \" + \"*\" * 21)  # 打印调试分隔符\n",
    "        print()\n",
    "\n",
    "    return content, new_msg_history  # 返回生成的内容和更新后的消息历史记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")   \n",
    "client.base_url = os.getenv(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "content, his = get_response_from_llm(msg='hello', client=client, model='gpt-4o-2024-05-13', system_message=\"You are a helpful AI assistant.\")\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hello! How can I assist you today?',\n",
       "  'Hello! How can I assist you today?',\n",
       "  'Hello! How can I assist you today?'],\n",
       " [[{'role': 'user', 'content': 'hello'},\n",
       "   {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}],\n",
       "  [{'role': 'user', 'content': 'hello'},\n",
       "   {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}],\n",
       "  [{'role': 'user', 'content': 'hello'},\n",
       "   {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch_responses_from_llm(msg='hello', client=client, model='gpt-4o-2024-05-13', system_message=\"You are a helpful AI assistant.\",n_responses=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_between_markers(llm_output):\n",
    "    # 定义 JSON 开始和结束的标记\n",
    "    json_start_marker = \"```json\"\n",
    "    json_end_marker = \"```\"\n",
    "\n",
    "    # 找到 JSON 字符串的开始和结束索引\n",
    "    start_index = llm_output.find(json_start_marker)\n",
    "    if start_index != -1:\n",
    "        start_index += len(json_start_marker)  # 将起始索引移动到标记之后的位置\n",
    "        end_index = llm_output.find(json_end_marker, start_index)\n",
    "    else:\n",
    "        return None  # 如果没有找到开始标记，则返回 None\n",
    "\n",
    "    if end_index == -1:\n",
    "        return None  # 如果没有找到结束标记，则返回 None\n",
    "\n",
    "    # 提取 JSON 字符串\n",
    "    json_string = llm_output[start_index:end_index].strip()  # 去除前后空格\n",
    "    try:\n",
    "        parsed_json = json.loads(json_string)  # 尝试解析 JSON 字符串\n",
    "        return parsed_json  # 如果成功，返回解析后的 JSON 对象\n",
    "    except json.JSONDecodeError:\n",
    "        return None  # 如果解析失败（无效的 JSON 格式），返回 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import get_response_from_llm\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "content, his = get_response_from_llm(msg='hello', client=client, model='gpt-4o-2024-05-13', system_message=\"You are a helpful AI assistant.\")\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
