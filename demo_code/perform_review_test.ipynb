{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "from llm import (\n",
    "    get_response_from_llm,\n",
    "    get_batch_responses_from_llm,\n",
    "    extract_json_between_markers,\n",
    ")\n",
    "import pprint\n",
    "import openai\n",
    "\n",
    "from prompt import *\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paper(pdf_path, num_pages=None, min_size=100):\n",
    "    \"\"\"\n",
    "    从 PDF 文件中加载文本。\n",
    "\n",
    "    参数:\n",
    "    pdf_path (str): PDF 文件的路径。\n",
    "    num_pages (int, optional): 要加载的页数。如果为 None，则加载所有页。\n",
    "    min_size (int, optional): 文本的最小长度。如果文本长度小于此值，则抛出异常。\n",
    "\n",
    "    返回:\n",
    "    str: 提取的文本。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 尝试使用 pymupdf4llm 库\n",
    "        if num_pages is None:\n",
    "            text = pymupdf4llm.to_markdown(pdf_path)\n",
    "        else:\n",
    "            reader = PdfReader(pdf_path)\n",
    "            min_pages = min(len(reader.pages), num_pages)\n",
    "            text = pymupdf4llm.to_markdown(pdf_path, pages=list(range(min_pages)))\n",
    "        if len(text) < min_size:\n",
    "            raise Exception(\"Text too short\")\n",
    "    except Exception as e:\n",
    "        # 如果 pymupdf4llm 库失败，打印错误并尝试使用 pymupdf 库\n",
    "        print(f\"Error with pymupdf4llm, falling back to pymupdf: {e}\")\n",
    "        try:\n",
    "            doc = pymupdf.open(pdf_path)  # 打开文档\n",
    "            if num_pages:\n",
    "                doc = doc[:num_pages]\n",
    "            text = \"\"\n",
    "            for page in doc:  # 遍历文档页\n",
    "                text = text + page.get_text()  # 获取 UTF-8 编码的纯文本\n",
    "            if len(text) < min_size:\n",
    "                raise Exception(\"Text too short\")\n",
    "        except Exception as e:\n",
    "            # 如果 pymupdf 库失败，打印错误并尝试使用 pypdf 库\n",
    "            print(f\"Error with pymupdf, falling back to pypdf: {e}\")\n",
    "            reader = PdfReader(pdf_path)\n",
    "            if num_pages is None:\n",
    "                text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "            else:\n",
    "                text = \"\".join(page.extract_text() for page in reader.pages[:num_pages])\n",
    "            if len(text) < min_size:\n",
    "                raise Exception(\"Text too short\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(path):\n",
    "    with open(path, \"r\") as json_file:\n",
    "        loaded = json.load(json_file)\n",
    "    return loaded[\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_fewshot_examples(num_fs_examples=1):\n",
    "    dir_path = os.getcwd()\n",
    "    fewshot_papers = [\n",
    "        os.path.join(dir_path, \"fewshot_examples/132_automated_relational.pdf\"),\n",
    "        os.path.join(dir_path, \"fewshot_examples/attention.pdf\"),\n",
    "        os.path.join(dir_path, \"fewshot_examples/2_carpe_diem.pdf\"),\n",
    "    ]\n",
    "\n",
    "    fewshot_reviews = [\n",
    "        os.path.join(dir_path, \"fewshot_examples/132_automated_relational.json\"),\n",
    "        os.path.join(dir_path, \"fewshot_examples/attention.json\"),\n",
    "        os.path.join(dir_path, \"fewshot_examples/2_carpe_diem.json\"),\n",
    "    ]\n",
    "    fewshot_prompt = \"\"\"\n",
    "Below are some sample reviews, copied from previous machine learning conferences.\n",
    "Note that while each review is formatted differently according to each reviewer's style, the reviews are well-structured and therefore easy to navigate.\n",
    "\"\"\"\n",
    "    for paper, review in zip(\n",
    "        fewshot_papers[:num_fs_examples], fewshot_reviews[:num_fs_examples]\n",
    "    ):\n",
    "        txt_path = paper.replace(\".pdf\", \".txt\")\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, \"r\") as f:\n",
    "                paper_text = f.read()\n",
    "        else:\n",
    "            paper_text = load_paper(paper)\n",
    "        review_text = load_review(review)\n",
    "        fewshot_prompt += f\"\"\"\n",
    "Paper:\n",
    "\n",
    "```\n",
    "{paper_text}\n",
    "```\n",
    "\n",
    "Review:\n",
    "\n",
    "```\n",
    "{review_text}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    return fewshot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_review(model, client, temperature, reviews):\n",
    "    # Write a meta-review from a set of individual reviews\n",
    "    review_text = \"\"\n",
    "    for i, r in enumerate(reviews):\n",
    "        review_text += f\"\"\"\n",
    "Review {i + 1}/{len(reviews)}:\n",
    "```\n",
    "{json.dumps(r)}\n",
    "```\n",
    "\"\"\"\n",
    "    base_prompt = neurips_form + review_text\n",
    "\n",
    "    llm_review, msg_history = get_response_from_llm(\n",
    "        base_prompt,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        system_message=meta_reviewer_system_prompt.format(reviewer_count=len(reviews)),\n",
    "        print_debug=False,\n",
    "        msg_history=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    meta_review = extract_json_between_markers(llm_review)\n",
    "    return meta_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_review(\n",
    "    text,  # 待评审的论文文本\n",
    "    model,  # 使用的语言模型\n",
    "    client,  # 客户端对象，用于与语言模型通信\n",
    "    num_reflections=1,  # 反思的次数，用于进一步优化评审结果\n",
    "    num_fs_examples=1,  # 少量示例的数量\n",
    "    num_reviews_ensemble=1,  # 集成评审的数量，决定生成多少个独立评审进行合并\n",
    "    temperature=0.75,  # 温度参数，控制生成文本的随机性\n",
    "    msg_history=None,  # 消息历史，用于跟踪生成过程中的对话历史\n",
    "    return_msg_history=False,  # 是否返回消息历史\n",
    "    reviewer_system_prompt=reviewer_system_prompt_neg,  # 评审生成时使用的系统提示\n",
    "    review_instruction_form=neurips_form,  # 评审指令的表单\n",
    "):\n",
    "    # 如果指定了少量示例，生成相应的提示文本\n",
    "    if num_fs_examples > 0:\n",
    "        fs_prompt = get_review_fewshot_examples(num_fs_examples)\n",
    "        base_prompt = review_instruction_form + fs_prompt\n",
    "    else:\n",
    "        base_prompt = review_instruction_form\n",
    "    # 在提示文本中添加需要评审的论文内容\n",
    "    base_prompt += f\"\"\"\n",
    "Here is the paper you are asked to review:\n",
    "```\n",
    "{text}\n",
    "```\"\"\"\n",
    "    # 如果集成评审数量大于1，进行多次评审并合并结果\n",
    "    if num_reviews_ensemble > 1:\n",
    "        llm_review, msg_histories = get_batch_responses_from_llm(\n",
    "            base_prompt,\n",
    "            model=model,\n",
    "            client=client,\n",
    "            system_message=reviewer_system_prompt,\n",
    "            print_debug=False,\n",
    "            msg_history=msg_history,\n",
    "            # Higher temperature to encourage diversity.\n",
    "            temperature=0.75,\n",
    "            n_responses=num_reviews_ensemble,\n",
    "        )\n",
    "        parsed_reviews = []\n",
    "        for idx, rev in enumerate(llm_review):\n",
    "            try:\n",
    "                parsed_reviews.append(extract_json_between_markers(rev))\n",
    "            except Exception as e:\n",
    "                print(f\"Ensemble review {idx} failed: {e}\")\n",
    "        parsed_reviews = [r for r in parsed_reviews if r is not None]\n",
    "        review = get_meta_review(model, client, temperature, parsed_reviews)\n",
    "\n",
    "        # take first valid in case meta-reviewer fails\n",
    "        if review is None:\n",
    "            review = parsed_reviews[0]\n",
    "\n",
    "        # Replace numerical scores with the average of the ensemble.\n",
    "        for score, limits in [\n",
    "            (\"Originality\", (1, 4)),\n",
    "            (\"Quality\", (1, 4)),\n",
    "            (\"Clarity\", (1, 4)),\n",
    "            (\"Significance\", (1, 4)),\n",
    "            (\"Soundness\", (1, 4)),\n",
    "            (\"Presentation\", (1, 4)),\n",
    "            (\"Contribution\", (1, 4)),\n",
    "            (\"Overall\", (1, 10)),\n",
    "            (\"Confidence\", (1, 5)),\n",
    "        ]:\n",
    "            scores = []\n",
    "            for r in parsed_reviews:\n",
    "                if score in r and limits[1] >= r[score] >= limits[0]:\n",
    "                    scores.append(r[score])\n",
    "            review[score] = int(round(np.mean(scores)))\n",
    "\n",
    "        # Rewrite the message history with the valid one and new aggregated review.\n",
    "        msg_history = msg_histories[0][:-1]\n",
    "        msg_history += [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"\"\"\n",
    "THOUGHT:\n",
    "I will start by aggregating the opinions of {num_reviews_ensemble} reviewers that I previously obtained.\n",
    "\n",
    "REVIEW JSON:\n",
    "```json\n",
    "{json.dumps(review)}\n",
    "```\n",
    "\"\"\",\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        llm_review, msg_history = get_response_from_llm(\n",
    "            base_prompt,\n",
    "            model=model,\n",
    "            client=client,\n",
    "            system_message=reviewer_system_prompt,\n",
    "            print_debug=False,\n",
    "            msg_history=msg_history,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        review = extract_json_between_markers(llm_review)\n",
    "\n",
    "    if num_reflections > 1:\n",
    "        for j in range(num_reflections - 1):\n",
    "            # print(f\"Relection: {j + 2}/{num_reflections}\")\n",
    "            text, msg_history = get_response_from_llm(\n",
    "                reviewer_reflection_prompt,\n",
    "                client=client,\n",
    "                model=model,\n",
    "                system_message=reviewer_system_prompt,\n",
    "                msg_history=msg_history,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            review = extract_json_between_markers(text)\n",
    "            assert review is not None, \"Failed to extract JSON from LLM output\"\n",
    "\n",
    "            if \"I am done\" in text:\n",
    "                # print(f\"Review generation converged after {j + 2} iterations.\")\n",
    "                break\n",
    "\n",
    "    if return_msg_history:\n",
    "        return review, msg_history\n",
    "    else:\n",
    "        return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_improvement(review, coder):\n",
    "    improvement_prompt = '''The following review has been created for your research paper:\n",
    "\"\"\"\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "Improve the text using the review.'''.format(\n",
    "        review=json.dumps(review)\n",
    "    )\n",
    "    coder_out = coder.run(improvement_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Summary': 'The paper investigates the impact of different weight '\n",
      "            'initialization strategies on the grokking phenomenon in '\n",
      "            'Transformer models, focusing on arithmetic tasks in finite '\n",
      "            'fields. It systematically compares five initialization methods '\n",
      "            '(PyTorch default, Xavier, He, Orthogonal, and Kaiming Normal) and '\n",
      "            'evaluates their effects on convergence speed and generalization '\n",
      "            'performance. The study employs a small Transformer architecture '\n",
      "            'for controlled experiments and uses statistical validation to '\n",
      "            'ensure robustness. The results show that Xavier initialization '\n",
      "            'consistently outperforms others, providing insights into '\n",
      "            'optimizing model training and improving generalization.',\n",
      " 'Strengths': ['Provides a thorough comparative study of weight initialization '\n",
      "               'strategies on grokking.',\n",
      "               'Employs a controlled experimental setup with statistical '\n",
      "               'validation to ensure robust findings.',\n",
      "               'Offers practical insights into optimizing training strategies '\n",
      "               'for Transformer models.',\n",
      "               'Well-organized and clearly written, with detailed methodology '\n",
      "               'and analysis.'],\n",
      " 'Weaknesses': ['The study is limited to a small Transformer architecture, '\n",
      "                'which may affect scalability.',\n",
      "                'Focuses on arithmetic tasks in finite fields, potentially '\n",
      "                'limiting generalizability to more complex tasks.',\n",
      "                'Limited discussion on the real-world applications and broader '\n",
      "                'implications of the findings.',\n",
      "                'Lacks theoretical insights into why certain initialization '\n",
      "                'strategies are more effective.'],\n",
      " 'Originality': 3,\n",
      " 'Quality': 3,\n",
      " 'Clarity': 4,\n",
      " 'Significance': 3,\n",
      " 'Questions': ['How do you anticipate the results will scale to larger '\n",
      "               'Transformer models and more complex tasks?',\n",
      "               'Have you considered the impact of other hyperparameters '\n",
      "               'besides initialization on grokking?',\n",
      "               'Could you provide more discussion on potential real-world '\n",
      "               'applications of these findings?',\n",
      "               'What are the potential limitations in applying these findings '\n",
      "               'to real-world tasks?'],\n",
      " 'Limitations': ['The experimental setup uses a small Transformer model, which '\n",
      "                 'may not directly scale to larger models.',\n",
      "                 'The tasks are limited to arithmetic operations in finite '\n",
      "                 'fields, which may not represent real-world complexities.',\n",
      "                 'Theoretical understanding of grokking is still lacking, '\n",
      "                 'requiring further investigation.'],\n",
      " 'Ethical Concerns': False,\n",
      " 'Soundness': 3,\n",
      " 'Presentation': 3,\n",
      " 'Contribution': 3,\n",
      " 'Overall': 6,\n",
      " 'Confidence': 4,\n",
      " 'Decision': 'Accept'}\n"
     ]
    }
   ],
   "source": [
    "text = load_paper(\"../example_papers/weight_initialization_grokking.pdf\")\n",
    "model = 'gpt-4o-2024-08-06'\n",
    "client = openai.OpenAI()\n",
    "\n",
    "review = perform_review(text, model, client, num_fs_examples=0, num_reviews_ensemble=3, num_reflections=2, reviewer_system_prompt=reviewer_system_prompt_base)\n",
    "\n",
    "pprint.pp(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
