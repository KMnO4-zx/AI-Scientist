{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "from typing import List, Dict, Union\n",
    "from llm import get_response_from_llm, extract_json_between_markers\n",
    "import openai\n",
    "import requests\n",
    "import backoff\n",
    "\n",
    "from prompt import idea_first_prompt, idea_reflection_prompt, novelty_prompt, novelty_system_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_API_KEY = os.getenv(\"S2_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ideas(\n",
    "    base_dir,\n",
    "    client,\n",
    "    model,\n",
    "    skip_generation=False,\n",
    "    max_num_generations=20,\n",
    "    num_reflections=5,\n",
    "):\n",
    "    # 如果 skip_generation 为真，则跳过生成过程并从文件中加载现有的想法\n",
    "    if skip_generation:\n",
    "        try:\n",
    "            with open(osp.join(base_dir, \"ideas.json\"), \"r\") as f:\n",
    "                ideas = json.load(f)\n",
    "            print(\"Loaded existing ideas:\")\n",
    "            for idea in ideas:\n",
    "                print(idea)\n",
    "            return ideas  # 返回从文件中加载的想法\n",
    "        except FileNotFoundError:\n",
    "            print(\"No existing ideas found. Generating new ideas.\")  # 文件不存在\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding existing ideas. Generating new ideas.\")  # JSON 解码错误\n",
    "\n",
    "    # 初始化一个存储想法的列表\n",
    "    idea_str_archive = []\n",
    "    \n",
    "    # 从文件中加载种子想法并将其转换为字符串格式\n",
    "    with open(osp.join(base_dir, \"seed_ideas.json\"), \"r\") as f:\n",
    "        seed_ideas = json.load(f)\n",
    "    for seed_idea in seed_ideas:\n",
    "        idea_str_archive.append(json.dumps(seed_idea))\n",
    "\n",
    "    # 读取包含实验代码的文件内容\n",
    "    with open(osp.join(base_dir, \"experiment.py\"), \"r\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    # 读取包含提示信息的文件内容\n",
    "    with open(osp.join(base_dir, \"prompt.json\"), \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # 提取系统提示\n",
    "    idea_system_prompt = prompt[\"system\"]\n",
    "\n",
    "    # 开始生成想法，最多生成 max_num_generations 次\n",
    "    for _ in range(max_num_generations):\n",
    "        print()\n",
    "        print(f\"Generating idea {_ + 1}/{max_num_generations}\")\n",
    "        try:\n",
    "            # 将之前生成的想法转化为字符串\n",
    "            prev_ideas_string = \"\\n\\n\".join(idea_str_archive)\n",
    "\n",
    "            # 消息历史初始化为空\n",
    "            msg_history = []\n",
    "            print(f\"Iteration 1/{num_reflections}\")\n",
    "            \n",
    "            # 使用 LLM 生成新的想法\n",
    "            text, msg_history = get_response_from_llm(\n",
    "                idea_first_prompt.format(\n",
    "                    task_description=prompt[\"task_description\"],\n",
    "                    code=code,\n",
    "                    prev_ideas_string=prev_ideas_string,\n",
    "                    num_reflections=num_reflections,\n",
    "                ),\n",
    "                client=client,\n",
    "                model=model,\n",
    "                system_message=idea_system_prompt,\n",
    "                msg_history=msg_history,\n",
    "            )\n",
    "            \n",
    "            # 解析输出，尝试从中提取 JSON 数据\n",
    "            json_output = extract_json_between_markers(text)\n",
    "            assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
    "            print(json_output)\n",
    "\n",
    "            # 如果反思次数大于1，则进行多次迭代改进\n",
    "            if num_reflections > 1:\n",
    "                for j in range(num_reflections - 1):\n",
    "                    print(f\"Iteration {j + 2}/{num_reflections}\")\n",
    "                    text, msg_history = get_response_from_llm(\n",
    "                        idea_reflection_prompt.format(\n",
    "                            current_round=j + 2, num_reflections=num_reflections\n",
    "                        ),\n",
    "                        client=client,\n",
    "                        model=model,\n",
    "                        system_message=idea_system_prompt,\n",
    "                        msg_history=msg_history,\n",
    "                    )\n",
    "                    # 再次解析输出，尝试从中提取 JSON 数据\n",
    "                    json_output = extract_json_between_markers(text)\n",
    "                    assert (\n",
    "                        json_output is not None\n",
    "                    ), \"Failed to extract JSON from LLM output\"\n",
    "                    print(json_output)\n",
    "\n",
    "                    # 如果输出中包含 \"I am done\" 字样，则认为已收敛，提前退出循环\n",
    "                    if \"I am done\" in text:\n",
    "                        print(f\"Idea generation converged after {j + 2} iterations.\")\n",
    "                        break\n",
    "\n",
    "            # 将新生成的想法加入存档\n",
    "            idea_str_archive.append(json.dumps(json_output))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate idea: {e}\")  # 捕获异常并打印错误信息\n",
    "            continue\n",
    "\n",
    "    # 保存生成的所有想法到文件\n",
    "    ideas = []\n",
    "    for idea_str in idea_str_archive:\n",
    "        ideas.append(json.loads(idea_str))\n",
    "\n",
    "    with open(osp.join(base_dir, \"ideas.json\"), \"w\") as f:\n",
    "        json.dump(ideas, f, indent=4)\n",
    "\n",
    "    return ideas  # 返回生成的想法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_idea(\n",
    "    base_dir,\n",
    "    client,\n",
    "    model,\n",
    "    prev_idea_archive=[],\n",
    "    num_reflections=5,\n",
    "    max_attempts=10,\n",
    "):\n",
    "    # 初始化想法存档，并获取当前存档的大小\n",
    "    idea_archive = prev_idea_archive\n",
    "    original_archive_size = len(idea_archive)\n",
    "\n",
    "    print(f\"Generating idea {original_archive_size + 1}\")\n",
    "\n",
    "    # 如果存档为空，则加载种子想法\n",
    "    if len(prev_idea_archive) == 0:\n",
    "        print(f\"First iteration, taking seed ideas\")\n",
    "        with open(osp.join(base_dir, \"seed_ideas.json\"), \"r\") as f:\n",
    "            seed_ideas = json.load(f)\n",
    "        for seed_idea in seed_ideas[:1]:  # 仅加载第一个种子想法\n",
    "            idea_archive.append(seed_idea)\n",
    "    else:\n",
    "        # 否则，从文件中读取实验代码和提示\n",
    "        with open(osp.join(base_dir, \"experiment.py\"), \"r\") as f:\n",
    "            code = f.read()\n",
    "        with open(osp.join(base_dir, \"prompt.json\"), \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "        idea_system_prompt = prompt[\"system\"]\n",
    "\n",
    "        # 尝试生成想法，最多进行 max_attempts 次尝试\n",
    "        for _ in range(max_attempts):\n",
    "            try:\n",
    "                # 将现有想法转换为字符串形式\n",
    "                idea_strings = []\n",
    "                for idea in idea_archive:\n",
    "                    idea_strings.append(json.dumps(idea))\n",
    "                prev_ideas_string = \"\\n\\n\".join(idea_strings)\n",
    "\n",
    "                # 初始化消息历史\n",
    "                msg_history = []\n",
    "                print(f\"Iteration 1/{num_reflections}\")\n",
    "                \n",
    "                # 使用 LLM 生成新的想法\n",
    "                text, msg_history = get_response_from_llm(\n",
    "                    idea_first_prompt.format(\n",
    "                        task_description=prompt[\"task_description\"],\n",
    "                        code=code,\n",
    "                        prev_ideas_string=prev_ideas_string,\n",
    "                        num_reflections=num_reflections,\n",
    "                    )\n",
    "                    + \"\"\"\n",
    "Completed ideas have an additional \"Score\" field which indicates the assessment by an expert ML reviewer.\n",
    "This is on a standard 1-10 ML conference scale.\n",
    "Scores of 0 indicate the idea failed either during experimentation, writeup or reviewing.\n",
    "\"\"\",\n",
    "                    client=client,\n",
    "                    model=model,\n",
    "                    system_message=idea_system_prompt,\n",
    "                    msg_history=msg_history,\n",
    "                )\n",
    "                \n",
    "                # 解析输出，尝试从中提取 JSON 数据\n",
    "                json_output = extract_json_between_markers(text)\n",
    "                assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
    "                print(json_output)\n",
    "\n",
    "                # 如果反思次数大于1，则进行多次迭代改进\n",
    "                if num_reflections > 1:\n",
    "                    for j in range(num_reflections - 1):\n",
    "                        print(f\"Iteration {j + 2}/{num_reflections}\")\n",
    "                        text, msg_history = get_response_from_llm(\n",
    "                            idea_reflection_prompt.format(\n",
    "                                current_round=j + 2, num_reflections=num_reflections\n",
    "                            ),\n",
    "                            client=client,\n",
    "                            model=model,\n",
    "                            system_message=idea_system_prompt,\n",
    "                            msg_history=msg_history,\n",
    "                        )\n",
    "                        \n",
    "                        # 再次解析输出，尝试从中提取 JSON 数据\n",
    "                        json_output = extract_json_between_markers(text)\n",
    "                        assert (\n",
    "                            json_output is not None\n",
    "                        ), \"Failed to extract JSON from LLM output\"\n",
    "                        print(json_output)\n",
    "\n",
    "                        # 如果输出中包含 \"I am done\" 字样，则认为已收敛，提前退出循环\n",
    "                        if \"I am done\" in text:\n",
    "                            print(\n",
    "                                f\"Idea generation converged after {j + 2} iterations.\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                # 将新生成的想法加入存档\n",
    "                idea_archive.append(json_output)\n",
    "                break  # 成功生成想法，退出尝试循环\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate idea: {e}\")  # 捕获异常并打印错误信息\n",
    "                continue\n",
    "\n",
    "    # 保存生成的所有想法到文件\n",
    "    with open(osp.join(base_dir, \"ideas.json\"), \"w\") as f:\n",
    "        json.dump(idea_archive, f, indent=4)\n",
    "\n",
    "    return idea_archive  # 返回更新后的想法存档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_backoff(details):\n",
    "    print(\n",
    "        f\"Backing off {details['wait']:0.1f} seconds after {details['tries']} tries \"\n",
    "        f\"calling function {details['target'].__name__} at {time.strftime('%X')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating idea 1/20\n",
      "Iteration 1/5\n",
      "{'Name': 'parameter_sharing', 'Title': 'Exploring Parameter Sharing in Character-Level Transformer Models', 'Experiment': 'Modify the Block class to optionally share parameters between some of the transformer blocks. Introduce a new configuration parameter to control the sharing size. Adjust the training loop to monitor convergence speed and final performance with shared vs non-shared configurations. Compare model performance, parameter size, and training time.', 'Interestingness': 7, 'Feasibility': 7, 'Novelty': 5}\n",
      "Iteration 2/5\n",
      "{'Name': 'parameter_sharing', 'Title': 'Exploring Parameter Sharing in Character-Level Transformer Models', 'Experiment': 'Modify the Block class to optionally share parameters between some of the transformer blocks. Introduce a new configuration parameter to control the sharing size. Adjust the training loop to monitor convergence speed and final performance with shared vs non-shared configurations. Compare model performance, parameter size, and training time.', 'Interestingness': 7, 'Feasibility': 7, 'Novelty': 5}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 2/20\n",
      "Iteration 1/5\n",
      "{'Name': 'knowledge_distillation', 'Title': 'Knowledge Distillation for Enhancing Character-Level Language Models', 'Experiment': \"Implement a knowledge distillation framework where a small language model (student) learns from a larger pre-trained model (teacher). Modify the training loop to incorporate a distillation loss, which involves minimizing the difference between the student's outputs and the teacher's soft labels. Compare the student model's performance with and without distillation in terms of accuracy, generalization, and inference speed.\", 'Interestingness': 8, 'Feasibility': 5, 'Novelty': 6}\n",
      "Iteration 2/5\n",
      "{'Name': 'knowledge_distillation', 'Title': 'Knowledge Distillation for Enhancing Character-Level Language Models', 'Experiment': \"Implement a knowledge distillation framework where a small language model (student) learns from a larger pre-trained model (teacher). Use an existing language model and adapt it to provide character-level outputs if necessary. Modify the training loop to incorporate a distillation loss, which involves minimizing the difference between the student's outputs and the teacher's soft labels. Compare the student model's performance with and without distillation in terms of accuracy, generalization, and inference speed.\", 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 6}\n",
      "Iteration 3/5\n",
      "{'Name': 'knowledge_distillation', 'Title': 'Knowledge Distillation for Enhancing Character-Level Language Models', 'Experiment': \"Implement a knowledge distillation framework where a small language model (student) learns from a larger pre-trained model (teacher). Use an existing language model and adapt it to provide character-level outputs if necessary. Modify the training loop to incorporate a distillation loss, which involves minimizing the difference between the student's outputs and the teacher's soft labels. Compare the student model's performance with and without distillation in terms of accuracy, generalization, and inference speed.\", 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 6}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 3/20\n",
      "Iteration 1/5\n",
      "{'Name': 'curriculum_learning', 'Title': 'Curriculum Learning: Progressive Complexity in Character-Level Model Training', 'Experiment': 'Implement a curriculum learning strategy where the training data is organized by increasing complexity. Modify the data loader to start training on simpler sequences (e.g., shorter sequences or sequences with common words) and gradually introduce more complex sequences as training progresses. Track and compare the convergence speed, final performance, and robustness of the model trained with and without curriculum learning.', 'Interestingness': 7, 'Feasibility': 5, 'Novelty': 6}\n",
      "Iteration 2/5\n",
      "{'Name': 'curriculum_learning', 'Title': 'Curriculum Learning: Progressive Complexity in Character-Level Model Training', 'Experiment': 'Implement a curriculum learning strategy where the training data is organized by increasing complexity. Modify the data loader to start training on simpler sequences (e.g., shorter sequences or sequences with common words) and gradually introduce more complex sequences as training progresses. Track and compare the convergence speed, final performance, and robustness of the model trained with and without curriculum learning.', 'Interestingness': 7, 'Feasibility': 5, 'Novelty': 6}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 4/20\n",
      "Iteration 1/5\n",
      "{'Name': 'attention_sparsity', 'Title': 'Exploring Attention Sparsity in Character-Level Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to implement sparse attention mechanisms. Introduce a new configuration parameter to control sparsity levels. Experiment with different sparsity patterns, such as fixed patterns or learnable sparsity masks. Compare the performance and computational efficiency of sparse vs dense attention models, considering metrics such as training time, final accuracy, and resource utilization.', 'Interestingness': 8, 'Feasibility': 5, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'attention_sparsity', 'Title': 'Exploring Attention Sparsity in Character-Level Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to implement sparse attention mechanisms. Introduce a new configuration parameter to control the sparsity level. Start with simple fixed sparsity patterns, such as block sparsity or random sparsity, before experimenting with learnable sparsity masks. Measure the impact on computational efficiency by logging GPU memory usage and inference speed, alongside model performance metrics such as training loss and accuracy. Compare these metrics against a baseline model with dense attention.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'attention_sparsity', 'Title': 'Exploring Attention Sparsity in Character-Level Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to implement sparse attention mechanisms. Introduce a new configuration parameter to control the sparsity level. Begin by experimenting with predefined sparsity patterns such as block sparsity or strided sparsity. Quantitatively evaluate the impact on computational efficiency using GPU memory usage and inference speed, as well as on model performance using metrics like training loss and accuracy. Compare these metrics against a baseline model with dense attention. If successful, explore learnable sparsity patterns as a subsequent phase.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}\n",
      "Iteration 4/5\n",
      "{'Name': 'attention_sparsity', 'Title': 'Exploring Attention Sparsity in Character-Level Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to implement sparse attention mechanisms. Introduce a new configuration parameter to control the sparsity level. Begin by experimenting with predefined sparsity patterns such as block sparsity or strided sparsity. Quantitatively evaluate the impact on computational efficiency using GPU memory usage and inference speed, as well as on model performance using metrics like training loss and accuracy. Compare these metrics against a baseline model with dense attention. If successful, explore learnable sparsity patterns as a subsequent phase.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 5/20\n",
      "Iteration 1/5\n",
      "{'Name': 'adaptive_computation_time', 'Title': 'Adaptive Computation Time in Character-Level Language Models', 'Experiment': 'Integrate an adaptive computation time mechanism into the transformer blocks. Introduce a halting unit in each block that decides whether further computation is needed based on the input. Adjust the training loop to include a regularization term for minimizing computation steps, and evaluate the impact on model efficiency and performance by comparing with the baseline model.', 'Interestingness': 8, 'Feasibility': 5, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'adaptive_computation_time', 'Title': 'Adaptive Computation Time in Character-Level Language Models', 'Experiment': 'Integrate a simplified adaptive computation time mechanism by adding a halting unit to a few selected transformer blocks. The halting unit will decide whether further computation is needed based on the input. Adjust the training loop to include a regularization term for minimizing computation steps, and evaluate the impact on model efficiency and performance by comparing with the baseline model.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'adaptive_computation_time', 'Title': 'Adaptive Computation Time in Character-Level Language Models', 'Experiment': 'Integrate a simplified adaptive computation time mechanism by adding a halting unit to a few selected transformer blocks. The halting unit will decide whether further computation is needed based on the input. Adjust the training loop to include a regularization term for minimizing computation steps. Evaluate the impact on model efficiency by measuring computation time and energy consumption and ensure performance by tracking accuracy against a baseline model. Allow tuning of the regularization term to balance efficiency and accuracy.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}\n",
      "Iteration 4/5\n",
      "{'Name': 'adaptive_computation_time', 'Title': 'Adaptive Computation Time in Character-Level Language Models', 'Experiment': 'Integrate a simplified adaptive computation time mechanism by adding a halting unit to a few selected transformer blocks. The halting unit will decide whether further computation is needed based on the input. Adjust the training loop to include a regularization term for minimizing computation steps. Evaluate the impact on model efficiency by measuring computation time and energy consumption and ensure performance by tracking accuracy against a baseline model. Allow tuning of the regularization term to balance efficiency and accuracy.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 6/20\n",
      "Iteration 1/5\n",
      "{'Name': 'noise_injection_training', 'Title': 'Enhancing Model Robustness and Generalization through Noise Injection', 'Experiment': \"Modify the training loop to introduce random noise at various stages: input noise by adding noise to input data, weight noise by perturbing weights after each update, and activation noise by adding noise to activations during forward passes. Implement separate configurations for each noise type, allowing for controlled experiments. Evaluate the model's performance on validation data with and without noise injection to determine the impact on generalization and robustness.\", 'Interestingness': 7, 'Feasibility': 7, 'Novelty': 6}\n",
      "Iteration 2/5\n",
      "{'Name': 'weight_noise_injection', 'Title': 'Improving Robustness and Generalization with Weight Noise Injection', 'Experiment': \"Modify the training loop to introduce random Gaussian noise to the model's weights after each optimization step. Implement a configuration parameter to control the noise level. Evaluate the model's performance on the validation data with and without noise injection to determine its impact on generalization. Additionally, test the model's robustness by introducing small perturbations to the input data during validation and measuring performance changes.\", 'Interestingness': 7, 'Feasibility': 8, 'Novelty': 5}\n",
      "Iteration 3/5\n",
      "{'Name': 'weight_noise_injection', 'Title': 'Improving Robustness and Generalization with Weight Noise Injection', 'Experiment': \"Modify the training loop to introduce random Gaussian noise to the model's weights after each optimization step. Implement a configuration parameter to control the noise level. Evaluate the model's performance on the validation data with and without noise injection to determine its impact on generalization. Additionally, test the model's robustness by introducing small perturbations to the input data during validation and measuring performance changes.\", 'Interestingness': 7, 'Feasibility': 8, 'Novelty': 5}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 7/20\n",
      "Iteration 1/5\n",
      "{'Name': 'mixed_precision_training', 'Title': 'Enhancing Training Efficiency with Mixed Precision in Character-Level Language Models', 'Experiment': \"Implement mixed precision training using PyTorch's automatic mixed precision (AMP). Modify the training loop to enable mixed precision and compare the training time, memory usage, and model performance against the baseline full precision training. Evaluate the impact on both training and inference speed and ensure model accuracy remains consistent.\", 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 5}\n",
      "Iteration 2/5\n",
      "{'Name': 'mixed_precision_training', 'Title': 'Enhancing Training Efficiency with Mixed Precision in Character-Level Language Models', 'Experiment': \"Implement mixed precision training using PyTorch's automatic mixed precision (AMP). Modify the training loop to enable mixed precision and compare the training time, memory usage, and model performance against the baseline full precision training. Evaluate the impact on both training and inference speed and ensure model accuracy remains consistent.\", 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 5}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 8/20\n",
      "Iteration 1/5\n",
      "{'Name': 'dynamic_tokenization', 'Title': 'Dynamic Tokenization Strategies for Character-Level Language Models', 'Experiment': 'Modify the data loader to implement dynamic tokenization strategies, starting with character-level granularity and shifting to subword-level or word-level granularity as training progresses. Implement a mechanism to control the transition based on training phase or performance metrics. Evaluate the impact on model performance, efficiency, and convergence speed compared to a static character-level tokenization approach.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'dynamic_tokenization', 'Title': 'Dynamic Tokenization Strategies for Character-Level Language Models', 'Experiment': 'Modify the data loader to implement dynamic tokenization strategies, starting with character-level granularity and shifting to subword-level or word-level granularity as training progresses. Implement a mechanism to control the transition based on training phase or performance metrics. Evaluate the impact on model performance, efficiency, and convergence speed compared to a static character-level tokenization approach.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 9/20\n",
      "Iteration 1/5\n",
      "{'Name': 'early_exit_strategy', 'Title': 'Early Exit Strategy: Efficient Inference in Character-Level Transformer Models', 'Experiment': 'Incorporate an early exit mechanism in the GPT model by adding a confidence score assessment within each Block. Modify the forward method to include a confidence check after each block, halting further computation if the confidence exceeds a threshold. Compare inference speed and accuracy with and without early exit to evaluate computational efficiency and model performance trade-offs.', 'Interestingness': 9, 'Feasibility': 5, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'early_exit_strategy', 'Title': 'Early Exit Strategy: Efficient Inference in Character-Level Transformer Models', 'Experiment': 'Incorporate an early exit mechanism in the GPT model by adding a confidence score assessment within each Block. Modify the forward method to include a confidence check after each block, using a simple metric such as the maximum softmax probability to decide on early exit. Compare inference speed and accuracy with and without early exit to evaluate computational efficiency and model performance trade-offs.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'early_exit_strategy', 'Title': 'Early Exit Strategy: Efficient Inference in Character-Level Transformer Models', 'Experiment': 'Incorporate an early exit mechanism in the GPT model by adding a confidence score assessment within each Block. Modify the forward method to include a confidence check after each block, using a simple metric such as the maximum softmax probability to decide on early exit. Compare inference speed and accuracy with and without early exit to evaluate computational efficiency and model performance trade-offs.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 7}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 10/20\n",
      "Iteration 1/5\n",
      "{'Name': 'adaptive_architecture_search', 'Title': 'Adaptive Architecture Search for Character-Level Language Models', 'Experiment': \"Implement a mechanism in the training loop that periodically evaluates model performance and adjusts the architecture based on predefined criteria. For example, start with fewer layers and gradually increase them if the validation loss plateaus, or adjust the embedding size based on validation performance. Compare the final architecture's performance, training time, and complexity against a static architecture baseline.\", 'Interestingness': 9, 'Feasibility': 5, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'adaptive_architecture_search', 'Title': 'Adaptive Architecture Search for Character-Level Language Models', 'Experiment': \"Implement a mechanism in the training loop that periodically evaluates model performance and adjusts the architecture based on predefined criteria. For example, start with fewer layers and gradually increase them if the validation loss plateaus, or adjust the embedding size based on validation performance. Compare the final architecture's performance, training time, and complexity against a static architecture baseline.\", 'Interestingness': 9, 'Feasibility': 5, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'adaptive_architecture_search', 'Title': 'Adaptive Architecture Search for Character-Level Language Models', 'Experiment': 'Integrate an adaptive mechanism in the training loop to periodically evaluate model performance and adjust specific architectural components, such as the number of layers or embedding size. Trigger adjustments based on predefined criteria like validation loss plateau or epoch milestones. Compare the performance, training time, and complexity of the final adaptive architecture against a static baseline.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 4/5\n",
      "{'Name': 'adaptive_architecture_search', 'Title': 'Adaptive Architecture Search for Character-Level Language Models', 'Experiment': 'Integrate an adaptive mechanism in the training loop to periodically evaluate model performance and adjust specific architectural components, such as the number of layers or embedding size. Trigger adjustments based on predefined criteria like validation loss plateau or epoch milestones. Compare the performance, training time, and complexity of the final adaptive architecture against a static baseline.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 11/20\n",
      "Iteration 1/5\n",
      "{'Name': 'memory_augmented_networks', 'Title': 'Enhancing Character-Level Language Models with Memory-Augmented Neural Networks', 'Experiment': \"Integrate a memory module into the GPT model architecture. Modify the GPT class to include an external memory component. Design read and write operations to interact with this memory during the forward pass. Adjust the training loop to update the memory states based on the gradient descent. Evaluate the model's performance on the validation set, focusing on tasks with long-range dependencies. Compare the performance, memory efficiency, and computational overhead with the baseline model.\", 'Interestingness': 9, 'Feasibility': 5, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'memory_augmented_networks', 'Title': 'Enhancing Character-Level Language Models with Memory-Augmented Neural Networks', 'Experiment': \"Integrate a simple memory buffer into the GPT model architecture. Modify the GPT class to include this memory component, which stores a fixed number of past hidden states. Design read operations to allow the model to access this buffer during the forward pass. Adjust the training loop to update the buffer's contents at each step. Evaluate the model's performance on the validation set, focusing on tasks with long-range dependencies. Compare the performance, memory efficiency, and computational overhead with the baseline model.\", 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'memory_augmented_networks', 'Title': 'Enhancing Character-Level Language Models with Memory-Augmented Neural Networks', 'Experiment': \"Integrate a parameterizable memory buffer into the GPT model architecture. Modify the GPT class to include this memory component, which stores a fixed number of past hidden states. Allow the buffer size to be tuned according to sequence length or compute resources. Design read operations to allow the model to access this buffer during the forward pass. Adjust the training loop to update the buffer's contents at each step. Evaluate the model's performance on the validation set, focusing on tasks with long-range dependencies. Compare the performance, memory efficiency, and computational overhead with the baseline model.\", 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 4/5\n",
      "{'Name': 'memory_augmented_networks', 'Title': 'Enhancing Character-Level Language Models with Memory-Augmented Neural Networks', 'Experiment': \"Integrate a parameterizable memory buffer into the GPT model architecture. Modify the GPT class to include this memory component, which stores a fixed number of past hidden states. Allow the buffer size to be tuned according to sequence length or compute resources. Design read operations to allow the model to access this buffer during the forward pass. Adjust the training loop to update the buffer's contents at each step. Evaluate the model's performance on the validation set, focusing on tasks with long-range dependencies. Compare the performance, memory efficiency, and computational overhead with the baseline model.\", 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 12/20\n",
      "Iteration 1/5\n",
      "{'Name': 'adaptive_dropout', 'Title': 'Adaptive Dropout: Dynamic Regularization in Character-Level Transformer Models', 'Experiment': 'Implement an adaptive dropout mechanism where the dropout rate is adjusted dynamically during training. Modify the training loop to track validation loss or other performance metrics, and adjust the dropout rate accordingly. For example, increase the dropout rate if validation loss decreases, suggesting potential overfitting. Compare the convergence speed, final validation performance, and robustness of the model with and without adaptive dropout.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6}\n",
      "Iteration 2/5\n",
      "{'Name': 'adaptive_dropout', 'Title': 'Adaptive Dropout: Dynamic Regularization in Character-Level Transformer Models', 'Experiment': 'Implement an adaptive dropout mechanism where the dropout rate is adjusted dynamically during training. Modify the training loop to track validation loss or other performance metrics, and adjust the dropout rate accordingly. For example, increase the dropout rate if validation loss decreases, suggesting potential overfitting. Compare the convergence speed, final validation performance, and robustness of the model with and without adaptive dropout.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 13/20\n",
      "Iteration 1/5\n",
      "{'Name': 'attention_temperature', 'Title': 'Exploring Attention Temperature in Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to incorporate an attention temperature parameter. Adjust the calculation of attention scores by dividing them by the temperature. Experiment with both fixed and dynamically adjusted temperatures based on validation performance. Compare the convergence speed, final performance, and robustness of the model with different temperature settings against the baseline model.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'attention_temperature', 'Title': 'Exploring Attention Temperature in Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to incorporate an attention temperature parameter. Adjust the calculation of attention scores by dividing them by the temperature. Experiment with both fixed and dynamically adjusted temperatures based on validation performance. Compare the convergence speed, final performance, and robustness of the model with different temperature settings against the baseline model.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 14/20\n",
      "Iteration 1/5\n",
      "{'Name': 'model_pruning', 'Title': 'Efficient Model Pruning in Character-Level Transformer Models', 'Experiment': 'Implement structured pruning within the training loop by identifying and removing less important weights or neurons. Modify the training loop to apply pruning at specific intervals, such as every few epochs. Evaluate the impact of pruning on model size, training time, and performance metrics, including validation loss and accuracy. Compare these metrics against the baseline model without pruning. Experiment with different pruning ratios and strategies to identify the optimal trade-off between model compactness and performance.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'model_pruning', 'Title': 'Efficient Model Pruning in Character-Level Transformer Models', 'Experiment': 'Implement structured pruning within the training loop by identifying and removing less important components such as heads in multi-head attention or neurons in MLP layers. Use metrics like weight magnitude or gradient-based importance to decide which components to prune. Modify the training loop to apply pruning at specific intervals, such as every few epochs. Evaluate the impact of pruning on model size, training time, and performance metrics, including validation loss and accuracy. Compare these metrics against the baseline model without pruning. Experiment with different pruning ratios and strategies to identify the optimal trade-off between model compactness and performance, ensuring the model maintains sufficient accuracy for practical use.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'model_pruning', 'Title': 'Efficient Model Pruning in Character-Level Transformer Models', 'Experiment': 'Implement structured pruning within the training loop by identifying and removing less important components such as heads in multi-head attention or neurons in MLP layers. Use metrics like weight magnitude or gradient-based importance to decide which components to prune. Modify the training loop to apply pruning at specific intervals, such as every few epochs. Evaluate the impact of pruning on model size, training time, and performance metrics, including validation loss and accuracy. Compare these metrics against the baseline model without pruning. Experiment with different pruning ratios and strategies to identify the optimal trade-off between model compactness and performance, ensuring the model maintains sufficient accuracy for practical use.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 15/20\n",
      "Iteration 1/5\n",
      "{'Name': 'multi_task_learning', 'Title': 'Multi-Task Learning in Character-Level Language Models', 'Experiment': 'Modify the data loader to generate labels for auxiliary tasks such as word boundary prediction or character categorization. Adjust the GPT model to include additional heads or output layers for these tasks. Update the training loop to compute auxiliary losses and integrate them with the main loss. Experiment with different weightings for the auxiliary losses to evaluate their impact on the main task performance. Track metrics such as training loss, validation loss, and accuracy to compare the multi-task model against the baseline.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'multi_task_learning', 'Title': 'Multi-Task Learning in Character-Level Language Models', 'Experiment': 'Modify the data loader to generate labels for auxiliary tasks such as word boundary prediction. Adjust the GPT model to include additional output layers for these tasks. Update the training loop to compute auxiliary losses and integrate them with the main loss. Carefully select auxiliary tasks that are directly related to the main task and measure their performance. Experiment with different weightings for the auxiliary losses to evaluate their impact on the main task performance. Track metrics such as training loss, validation loss, and accuracy to compare the multi-task model against the baseline.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'multi_task_learning', 'Title': 'Multi-Task Learning in Character-Level Language Models', 'Experiment': 'Modify the data loader to generate labels for auxiliary tasks such as word boundary prediction. Adjust the GPT model to include additional output layers for these tasks. Update the training loop to compute auxiliary losses and integrate them with the main loss. Carefully select auxiliary tasks that are directly related to the main task and measure their performance. Experiment with different weightings for the auxiliary losses to evaluate their impact on the main task performance. Track metrics such as training loss, validation loss, and accuracy to compare the multi-task model against the baseline.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 16/20\n",
      "Iteration 1/5\n",
      "{'Name': 'elastic_computation', 'Title': 'Elastic Computation: Dynamic Resource Allocation in Language Models', 'Experiment': 'Implement a mechanism in the model to dynamically adjust its computation resources based on input complexity or task difficulty. Modify the forward method to include a decision-making module that evaluates input complexity and adjusts model parameters like active layers, attention heads, or embedding sizes. Compare the efficiency (e.g., computation time, energy consumption) and performance (e.g., accuracy, loss) against a static model configuration.', 'Interestingness': 9, 'Feasibility': 5, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'elastic_computation', 'Title': 'Elastic Computation: Dynamic Layer Utilization in Language Models', 'Experiment': 'Implement a mechanism in the model to dynamically adjust the number of active layers based on input complexity. Modify the forward method to include a decision-making module that evaluates input complexity and adjusts the number of transformer blocks used. Compare the efficiency (e.g., computation time, energy consumption) and performance (e.g., accuracy, loss) against a static model configuration.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'elastic_computation', 'Title': 'Elastic Computation: Dynamic Layer Utilization in Language Models', 'Experiment': 'Implement a mechanism in the model to dynamically adjust the number of active layers based on input complexity. Modify the forward method to include a decision-making module that evaluates input complexity and adjusts the number of transformer blocks used. Compare the efficiency (e.g., computation time, energy consumption) and performance (e.g., accuracy, loss) against a static model configuration.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 17/20\n",
      "Iteration 1/5\n",
      "{'Name': 'data_noise_robustness', 'Title': 'Enhancing Model Robustness Against Data Corruption: A Noise Injection Approach', 'Experiment': \"Modify the data loader to introduce controlled noise to the input data during training. Implement a configuration parameter to control the noise level and type (e.g., Gaussian, salt-and-pepper). Evaluate the model's performance on a clean validation set with and without noise introduction during training to determine its impact on generalization and robustness. Track metrics such as validation loss and accuracy to compare the noisily trained model against a baseline trained on clean data.\", 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6}\n",
      "Iteration 2/5\n",
      "{'Name': 'data_noise_robustness', 'Title': 'Enhancing Model Robustness Against Data Corruption: A Noise Injection Approach', 'Experiment': \"Modify the data loader to introduce controlled noise to the input data during training. Implement a configuration parameter to control the noise level and type (e.g., Gaussian, salt-and-pepper). Conduct experiments to find the optimal noise level by evaluating the model's performance on both noisy and clean validation sets. Track metrics such as validation loss and accuracy to compare the noisily trained model against a baseline trained on clean data, ensuring that robustness does not compromise performance on clean data.\", 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6}\n",
      "Iteration 3/5\n",
      "{'Name': 'data_noise_robustness', 'Title': 'Enhancing Model Robustness Against Data Corruption: A Noise Injection Approach', 'Experiment': \"Modify the data loader to introduce controlled noise to the input data during training. Implement a configuration parameter to control the noise level and type (e.g., Gaussian, salt-and-pepper). Conduct experiments to find the optimal noise level by evaluating the model's performance on both noisy and clean validation sets. Track metrics such as validation loss and accuracy to compare the noisily trained model against a baseline trained on clean data, ensuring that robustness does not compromise performance on clean data.\", 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 18/20\n",
      "Iteration 1/5\n",
      "{'Name': 'character_data_augmentation', 'Title': 'Character-Level Data Augmentation for Robust Language Model Training', 'Experiment': 'Modify the data loader to implement character-level data augmentation strategies, such as random deletion, swapping, or insertion of characters. Introduce configuration parameters to control the augmentation probability and type. Evaluate the impact on model generalization by comparing the validation loss and accuracy against a baseline model trained without augmentation. Additionally, assess robustness by introducing noise to the validation set and comparing performance.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'character_data_augmentation', 'Title': 'Character-Level Data Augmentation for Robust Language Model Training', 'Experiment': 'Modify the data loader to implement character-level data augmentation strategies, such as random deletion, swapping, or insertion of characters. Introduce configuration parameters to control the augmentation probability and type. Evaluate the impact on model generalization by comparing the validation loss and accuracy against a baseline model trained without augmentation. Additionally, assess robustness by introducing noise to the validation set and comparing performance.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 19/20\n",
      "Iteration 1/5\n",
      "{'Name': 'meta_learning', 'Title': 'Meta-Learning Strategies for Rapid Adaptation in Character-Level Language Models', 'Experiment': \"Modify the training loop to incorporate meta-learning principles. Implement a meta-objective function to optimize the model parameters for adaptability. Use methods like Model-Agnostic Meta-Learning (MAML) to guide the optimization process. Simulate task variability using subsets of the current dataset or synthetic tasks. Evaluate the model's performance on new tasks and data distributions after meta-training, comparing adaptation speed and accuracy to a baseline model.\", 'Interestingness': 9, 'Feasibility': 5, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'adaptive_hyperparameters', 'Title': 'Adaptive Hyperparameter Tuning for Enhanced Learning in Character-Level Models', 'Experiment': 'Modify the training loop to dynamically adjust hyperparameters like learning rate and dropout based on performance metrics during training. Implement a feedback mechanism that tunes these hyperparameters in response to changes in training or validation loss. Compare model performance and adaptability with a baseline using static hyperparameters, focusing on metrics like convergence speed and final accuracy.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'adaptive_hyperparameters', 'Title': 'Adaptive Hyperparameter Tuning for Enhanced Learning in Character-Level Models', 'Experiment': 'Modify the training loop to dynamically adjust hyperparameters like learning rate and dropout based on performance metrics during training. Implement a feedback mechanism using simple heuristics or automated methods (e.g., Bayesian optimization) to tune these hyperparameters in response to changes in training or validation loss. Compare model performance and adaptability with a baseline using static hyperparameters, focusing on metrics like convergence speed and final accuracy.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 4/5\n",
      "{'Name': 'adaptive_hyperparameters', 'Title': 'Adaptive Hyperparameter Tuning for Enhanced Learning in Character-Level Models', 'Experiment': 'Modify the training loop to dynamically adjust hyperparameters like learning rate and dropout based on performance metrics during training. Implement a feedback mechanism using simple heuristics or automated methods (e.g., Bayesian optimization) to tune these hyperparameters in response to changes in training or validation loss. Compare model performance and adaptability with a baseline using static hyperparameters, focusing on metrics like convergence speed and final accuracy.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 20/20\n",
      "Iteration 1/5\n",
      "{'Name': 'attention_head_specialization', 'Title': 'Attention Head Specialization: Encouraging Diverse Information Processing in Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to include a regularization term that penalizes similar attention patterns across heads. Implement visualization tools to analyze attention patterns of each head. Evaluate model performance, including accuracy and inference speed, with and without head specialization. Compare attention pattern diversity and task-specific improvements.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'attention_head_specialization', 'Title': 'Attention Head Specialization: Encouraging Diverse Information Processing in Transformer Models', 'Experiment': 'Modify the CausalSelfAttention class to include a regularization term that penalizes similar attention patterns across heads using cosine similarity. Implement visualization tools to analyze attention patterns of each head. Evaluate model performance, including accuracy and inference speed, with and without head specialization. Compare attention pattern diversity and task-specific improvements.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"./template_data/nanoGPT_lite/\"\n",
    "client = openai.OpenAI()\n",
    "model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "ideas_generation = generate_ideas(base_dir=BASE_DIR, client=client, model=model, skip_generation=False, max_num_generations=20, num_reflections=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo, requests.exceptions.HTTPError, on_backoff=on_backoff\n",
    ")\n",
    "def search_for_papers(query, result_limit=10) -> Union[None, List[Dict]]:\n",
    "    # 检查查询字符串是否为空\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    # 发起 HTTP GET 请求，查询论文数据\n",
    "    rsp = requests.get(\n",
    "        \"https://api.semanticscholar.org/graph/v1/paper/search\",\n",
    "        headers={\"X-API-KEY\": S2_API_KEY},\n",
    "        params={\n",
    "            \"query\": query,\n",
    "            \"limit\": result_limit,\n",
    "            \"fields\": \"title,authors,venue,year,abstract,citationStyles,citationCount\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # 打印响应状态码\n",
    "    print(f\"Response Status Code: {rsp.status_code}\")\n",
    "    \n",
    "    # 打印响应内容的前500个字符\n",
    "    print(f\"Response Content: {rsp.text[:500]}\")\n",
    "    \n",
    "    # 如果响应状态码不是200，则引发HTTPError异常\n",
    "    rsp.raise_for_status()\n",
    "    \n",
    "    # 解析响应的 JSON 数据\n",
    "    results = rsp.json()\n",
    "    \n",
    "    # 获取总结果数\n",
    "    total = results[\"total\"]\n",
    "    \n",
    "    # 等待1秒钟，以防止频繁请求\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    # 如果没有结果，返回 None\n",
    "    if not total:\n",
    "        return None\n",
    "    \n",
    "    # 获取论文数据列表\n",
    "    papers = results[\"data\"]\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_idea_novelty(\n",
    "    ideas,  # 需要检查的想法列表\n",
    "    base_dir,  # 存放实验代码和提示的基础目录\n",
    "    client,  # 用于与LLM交互的客户端对象\n",
    "    model,  # 用于执行查询的LLM模型名称\n",
    "    max_num_iterations=10,  # 最大迭代次数，默认值为10\n",
    "):\n",
    "    # 读取实验代码文件 experiment.py\n",
    "    with open(osp.join(base_dir, \"experiment.py\"), \"r\") as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    # 读取提示文件 prompt.json\n",
    "    with open(osp.join(base_dir, \"prompt.json\"), \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "        task_description = prompt[\"task_description\"]  # 提取任务描述\n",
    "\n",
    "    # 遍历每个想法，检查其创新性\n",
    "    for idx, idea in enumerate(ideas):\n",
    "        if \"novel\" in idea:\n",
    "            print(f\"Skipping idea {idx}, already checked.\")  # 如果已经检查过，跳过\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nChecking novelty of idea {idx}: {idea['Name']}\")\n",
    "\n",
    "        novel = False  # 初始化标志，表示是否为创新\n",
    "        msg_history = []  # 消息历史，用于跟踪对话\n",
    "        papers_str = \"\"  # 存储找到的论文信息\n",
    "\n",
    "        for j in range(max_num_iterations):\n",
    "            try:\n",
    "                # 调用 LLM 获取响应\n",
    "                text, msg_history = get_response_from_llm(\n",
    "                    novelty_prompt.format(\n",
    "                        current_round=j + 1,  # 当前轮次\n",
    "                        num_rounds=max_num_iterations,  # 总轮次\n",
    "                        idea=idea,  # 当前想法\n",
    "                        last_query_results=papers_str,  # 上一轮查询结果\n",
    "                    ),\n",
    "                    client=client,\n",
    "                    model=model,\n",
    "                    system_message=novelty_system_msg.format(\n",
    "                        num_rounds=max_num_iterations,  # 总轮次\n",
    "                        task_description=task_description,  # 任务描述\n",
    "                        code=code,  # 实验代码\n",
    "                    ),\n",
    "                    msg_history=msg_history,  # 消息历史\n",
    "                )\n",
    "                \n",
    "                # 检查响应中是否包含“novel”或“not novel”的决策\n",
    "                if \"decision made: novel\" in text.lower():\n",
    "                    print(\"Decision made: novel after round\", j)\n",
    "                    novel = True\n",
    "                    break\n",
    "                if \"decision made: not novel\" in text.lower():\n",
    "                    print(\"Decision made: not novel after round\", j)\n",
    "                    break\n",
    "\n",
    "                # 解析输出中的 JSON 数据\n",
    "                json_output = extract_json_between_markers(text)\n",
    "                assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
    "\n",
    "                # 搜索相关论文\n",
    "                query = json_output[\"Query\"]  # 从 JSON 中获取查询字符串\n",
    "                papers = search_for_papers(query, result_limit=10)  # 查询论文\n",
    "                if papers is None:\n",
    "                    papers_str = \"No papers found.\"\n",
    "\n",
    "                # 将找到的论文格式化为字符串\n",
    "                paper_strings = []\n",
    "                for i, paper in enumerate(papers):\n",
    "                    paper_strings.append(\n",
    "                        \"\"\"{i}: {title}. {authors}. {venue}, {year}.\\nNumber of citations: {cites}\\nAbstract: {abstract}\"\"\".format(\n",
    "                            i=i,\n",
    "                            title=paper[\"title\"],\n",
    "                            authors=paper[\"authors\"],\n",
    "                            venue=paper[\"venue\"],\n",
    "                            year=paper[\"year\"],\n",
    "                            cites=paper[\"citationCount\"],\n",
    "                            abstract=paper[\"abstract\"],\n",
    "                        )\n",
    "                    )\n",
    "                papers_str = \"\\n\\n\".join(paper_strings)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        idea[\"novel\"] = novel  # 将创新性结果存入想法字典中\n",
    "\n",
    "    # 将结果保存到 JSON 文件中\n",
    "    results_file = osp.join(base_dir, \"ideas.json\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(ideas, f, indent=4)\n",
    "\n",
    "    return ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./template_data/nanoGPT_lite/\"\n",
    "client = openai.OpenAI()\n",
    "model = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking novelty of idea 0: adaptive_block_size\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 3399, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"d4b99821ab8c1ee3271a72dc4163feb8d310c8a0\", \"title\": \"DBPS: Dynamic Block Size and Precision Scaling for Efficient DNN Training Supported by RISC-V ISA Extensions\", \"abstract\": \"Over the past decade, it has been found that deep neural networks (DNNs) perform better on visual perception and language understanding tasks as their size increases. However, this comes at the cost of high energy consumption and large memory requirement to tr\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 732, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"36daf0578aad5d3180d531cccbd32d65d62c8317\", \"title\": \"Extending Context Window in Large Language Models with Segmented Base Adjustment for Rotary Position Embeddings\", \"abstract\": \"In the realm of large language models (LLMs), extending the context window for long text processing is crucial for enhancing performance. This paper introduces SBA-RoPE (Segmented Base Adjustment for Rotary Position Embeddings), a novel approach designed to \n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 1: layerwise_learning_rates\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 16512, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"e09bfc955fbf66e0a042ca3f921108b823093b2e\", \"title\": \"Layer-wise Learning Rate Optimization for Task-Dependent Fine-Tuning of Pre-trained Models: An Evolutionary Approach\", \"abstract\": \"The superior performance of large-scale pre-trained models, such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), has received increasing attention in both academic and industrial research\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 787, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"e09bfc955fbf66e0a042ca3f921108b823093b2e\", \"title\": \"Layer-wise Learning Rate Optimization for Task-Dependent Fine-Tuning of Pre-trained Models: An Evolutionary Approach\", \"abstract\": \"The superior performance of large-scale pre-trained models, such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), has received increasing attention in both academic and industrial research a\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 15302, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"c5d3677dc90370c2de868fe39ad29391c933cbe4\", \"title\": \"MoMo: Momentum Models for Adaptive Learning Rates\", \"abstract\": \"Training a modern machine learning architecture on a new task requires extensive learning-rate tuning, which comes at a high computational cost. Here we develop new Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well. We first develop MoMo, a Mom\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 2: parameter_sharing\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 546, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"ca6c3e5cb2f3b7f35377d90142302ef8044a2425\", \"title\": \"DistilALHuBERT: A Distilled Parameter Sharing Audio Representation Model\", \"abstract\": \"Self-supervised pre-trained audio representation models such as Wav2vec or HuBERT have brought notable improvements to many downstream audio-related tasks, but the huge number of parameters of these pre-trained models sets a barrier to their application on memory-constrained edge devices. Recursi\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 252, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"8e8a63ec5d517c64b8fc44843fe169f549fd3f57\", \"title\": \"Character-Level Chinese Backpack Language Models\", \"abstract\": \"The Backpack is a Transformer alternative shown to improve interpretability in English language modeling by decomposing predictions into a weighted sum of token sense components. However, Backpacks\\u2019 reliance on token-defined meaning raises questions as to their potential for languages other than English, a language\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 3: knowledge_distillation\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 27224, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"15043deecb4cc81ed8bf530df3848508dfbba4e3\", \"title\": \"Data Expansion for Named Entity Recognition based on migration learning\", \"abstract\": \"The pivotal role of named entity recognition in the field of natural language comprehension significantly influences numerous tasks, including the likes of machine translation, the distillation of events, and the fabrication of knowledge graphs. Confronting the issue of data scarcity, the method\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 4352, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"15043deecb4cc81ed8bf530df3848508dfbba4e3\", \"title\": \"Data Expansion for Named Entity Recognition based on migration learning\", \"abstract\": \"The pivotal role of named entity recognition in the field of natural language comprehension significantly influences numerous tasks, including the likes of machine translation, the distillation of events, and the fabrication of knowledge graphs. Confronting the issue of data scarcity, the method \n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=character-level+knowledge+distillation+language+models&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 4: curriculum_learning\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 18116, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"454525329ef2728de5187063f96d039435a51c2d\", \"title\": \"Continuation Curriculum Learning with Question Answering on the SQuAD Dataset\", \"abstract\": \"We implement a slightly simplified Bi-Directional Attention Flow Model[4] and a slightly modified Multi-Perspective Context Matching[6] model for Question Answering on the SQuAD dataset. In the Multi-Perspective model, we add perspective matching between forward and backward contexts. We o\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 56068, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a\", \"title\": \"Learning to Execute\", \"abstract\": \"Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditi\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 56068, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a\", \"title\": \"Learning to Execute\", \"abstract\": \"Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditi\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 5: attention_sparsity\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 23538, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"9b2ecd30caa9e69100a9a8ebcb77fbafcb1d3f27\", \"title\": \"Sparse Mix-Attention Transformer for Multispectral Image and Hyperspectral Image Fusion\", \"abstract\": \"Multispectral image (MSI) and hyperspectral image (HSI) fusion (MHIF) aims to address the challenge of acquiring high-resolution (HR) HSI images. This field combines a low-resolution (LR) HSI with an HR-MSI to reconstruct HR-HSIs. Existing methods directly utilize transformers to\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 353, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d\", \"title\": \"BiFormer: Vision Transformer with Bi-Level Routing Attention\", \"abstract\": \"As the core building block of vision transformers, attention is a powerful tool to capture long-range dependency. However, such power comes at a cost: it incurs a huge computation burden and heavy memory footprint as pairwise token interaction across all spatial locations is computed. A series of works attem\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 6: adaptive_computation_time\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 56474, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"45da759d5d89fa0ca59142615aac71cd6b2debc8\", \"title\": \"DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference\", \"abstract\": \"Large-scale pre-trained language models have shown remarkable results in diverse NLP applications. However, these performance gains have been accompanied by a significant increase in computation time and model size, stressing the need to develop new or complementary strategies to inc\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 12176, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"537a9ca1b98c58fc77cb175e049433b700c3404a\", \"title\": \"Attending to Mathematical Language with Transformers\", \"abstract\": \"Mathematical expressions were generated, evaluated and used to train neural network models based on the transformer architecture. The expressions and their targets were analyzed as a character-level sequence transduction task in which the encoder and decoder are built on attention mechanisms. Three models were tra\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 2841, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"fa1559a6a71ef88baf151359257352ac0bc7c90d\", \"title\": \"CharSS: Character-Level Transformer Model for Sanskrit Word Segmentation\", \"abstract\": \"Subword tokens in Indian languages inherently carry meaning, and isolating them can enhance NLP tasks, making sub-word segmentation a crucial process. Segmenting Sanskrit and other Indian languages into subtokens is not straightforward, as it may include sandhi, which may lead to changes in the \n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 7: weight_noise_injection\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 4958, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"a3db34d72eecf683fbbbedee147d5aad7f05b13c\", \"title\": \"UNIQ : Uniform Noise Injection for non-uniform Quantization of neural networks\", \"abstract\": \"We present a novel method for training a neural network amenable to inference in low-precision arithmetic with quantized weights and activations. The training is performed in full precision with random noise injection emulating quantization noise. In order to circumvent the need to simulat\n",
      "Decision made: not novel after round 1\n",
      "\n",
      "Checking novelty of idea 8: mixed_precision_training\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 3594, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"ebff0de12d826c3eea09cccf5f717f3ac3489366\", \"title\": \"APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models\", \"abstract\": \"Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precis\n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=mixed+precision+character-level+language+models&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 26, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"3c48445a3ecc228e8504491ac05c24d3c036d342\", \"title\": \"Online Nigerian Languages Word-Level Character Recognition Using Deep Transfer Learning (short paper)\", \"abstract\": \"The application of artificial intelligence has cut across all areas of life birthing from the technological advancement and its wide acceptance and usage. The conversion of typed image text into machine readable format has become necessity as majority of typing and com\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 9: dynamic_tokenization\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 4830, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"c20942cedd92dcc4e6270f85af780464da655c4c\", \"title\": \"Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models\", \"abstract\": \"This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across v\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 38741, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"3c477f0e43660ce1ba39c111df312929da37f81f\", \"title\": \"Efficient Domain Adaptation of Language Models via Adaptive Tokenization\", \"abstract\": \"Contextual embedding-based language models trained on large data sets, such as BERT and RoBERTa, provide strong performance across a wide range of tasks and are ubiquitous in modern NLP. It has been observed that fine-tuning these models on tasks involving data from domains different from that \n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 10: early_exit_strategy\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 561, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"81b7d8a9311c1f2d20fd1bc1d0d9835ce19d4b24\", \"title\": \"DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models\", \"abstract\": \"Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 77, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"81b7d8a9311c1f2d20fd1bc1d0d9835ce19d4b24\", \"title\": \"DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models\", \"abstract\": \"Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an \n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 11: adaptive_architecture_search\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 22025, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"82eb642c407308d8c6e1ccc5c4a870681a8f17dd\", \"title\": \"NAS-ASDet: An Adaptive Design Method for Surface Defect Detection Network using Neural Architecture Search\", \"abstract\": \"Deep convolutional neural networks (CNNs) have been widely used in surface defect detection. However, no CNN architecture is suitable for all detection tasks and designing effective task-specific requires considerable effort. The neural architecture search (NAS\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 8915, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\", \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\", \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 12: memory_augmented_networks\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 81523, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"9886220589ecbab14736a56655a1ae75f4d84da4\", \"title\": \"Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications\", \"abstract\": \"This paper explores Memory-Augmented Neural Networks (MANNs), delving into how they blend human-like memory processes into AI. It covers different memory types, like sensory, short-term, and long-term memory, linking psychological theories with AI applications. The study investigates a\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 7, \"offset\": 0, \"data\": [{\"paperId\": \"da4df70c7309af93adc39d064e698cb326ac9bee\", \"title\": \"RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent\", \"abstract\": \"Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce tox\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 13: adaptive_dropout\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 11659, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"a50d31c082521817a1e74cae584963a63163ca70\", \"title\": \"Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search\", \"abstract\": \"Despite transformers\\u2019 impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate model for each possible computational budget. In this paper, we extend \n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 14072, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"3c5ac53155bfed24b170f65da806e5c9715b6f03\", \"title\": \"Enhancing Pneumonia Detection using Vision Transformer with Dynamic Mapping Re-Attention Mechanism\", \"abstract\": \"Pneumonia is a prevalent respiratory infection with potentially life-threatening consequences. In this research, we propose a novel deep learning approach to enhance pneumonia detection using the Vision Transformer (ViT) architecture with a dynamic mapping re-attention\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 97016, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"f9f19bee621faf46f90b023f8de8248b57becbc4\", \"title\": \"Adaptive dropout for training deep neural networks\", \"abstract\": \"Recently, it was shown that deep neural networks can perform very well if the activities of hidden units are regularized during learning, e.g, by randomly dropping out 50% of their activities. We describe a method called 'standout' in which a binary belief network is overlaid on a neural network and is used to regul\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 14: attention_temperature\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 129863, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"bacb5382917e5e636748962d132d56aa59cb8374\", \"title\": \"Research on Transformer Temperature Rise Prediction and Fault Warning Based on Attention-GRU\", \"abstract\": \"In order to predict the temperature rise curve of power transformer groups and achieve temperature rise fault warning. Build a training and prediction set by measuring the load and top oil temperature of the transformer. The GRU time series model with improved attention mec\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 6897, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"c1738f21ea2460e1015d590906a4f43e155f60c8\", \"title\": \"The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit\", \"abstract\": \"In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limi\n",
      "Decision made: not novel after round 2\n",
      "\n",
      "Checking novelty of idea 15: model_pruning\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 1566, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"3ea79430455304c782572dfb6ca3e5230b0351de\", \"title\": \"GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer\", \"abstract\": \"The recently proposed Vision transformers (ViTs) have shown\\nvery impressive empirical performance in various computer vision tasks,\\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale s\n",
      "Decision made: not novel after round 1\n",
      "\n",
      "Checking novelty of idea 16: multi_task_learning\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 23778, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"29254a3f5d4ab0bca7719a1a3f5d26ba09a53284\", \"title\": \"Bootstrapping NLU Models with Multi-task Learning\", \"abstract\": \"Bootstrapping natural language understanding (NLU) systems with minimal training data is a fundamental challenge of extending digital assistants like Alexa and Siri to a new language. A common approach that is adapted in digital assistants when responding to a user query is to process the input in a pipeline manner w\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 14830, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"1f965c3440ef1d2a2ed60925b2554efdc08662bc\", \"title\": \"TEAM BIAS BUSTERS@LT-EDI-2023: Detecting Signs of Depression with Generative Pretrained Transformers\", \"abstract\": \"This paper describes our methodology adopted to participate in the multi-class classification task under the auspices of the Third Workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI) in the Recent Advances in Natural Language Processing (RANLP\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 30106, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"1961368c26f42f0e6777d34b6e9c40e179651bb1\", \"title\": \"Learning High-Quality and General-Purpose Phrase Representations\", \"abstract\": \"Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification.The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embe\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 17: elastic_computation\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 11436, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"04cca8e341a5da42b29b0bc831cb25a0f784fa01\", \"title\": \"Adaptive Computation Time for Recurrent Neural Networks\", \"abstract\": \"This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not a\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 131, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"53c53be8bd06ce5a538f60bb957dd4eea781b111\", \"title\": \"AI Processor with Sparsity-adaptive Real-time Dynamic Frequency Modulation for Convolutional Neural Networks and Transformers\", \"abstract\": \"A high degree of sparsity in deep learning models is regarded as a great opportunity to achieve aggressive energy and delay savings in both convolutional neural networks (e.g., sparsity: > 89% [1]) and Transformers (e.g., >75 [2]) by avoiding r\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 18: data_noise_robustness\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 5155, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"627187eee6b4421b2a8ce14fc61af51a728b7fbb\", \"title\": \"DINAR: Enabling Distribution Agnostic Noise Injection in Machine Learning Hardware\", \"abstract\": \"Machine learning (ML) has seen a major rise in popularity on edge devices in recent years, ranging from IoT devices to self-driving cars. Security in a critical consideration on these platforms. State-of-the-art security-centric ML algorithms (e.g., differentially private ML, adversari\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 9967, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"0ea0c0fdc61963c168b2738ee398197115e0e720\", \"title\": \"Noise Injection/ Machine Learning Fraud Detection Framework in Time Series Data\", \"abstract\": \"\\u2014Internet connectivity (IoT) of ever increasing numbers of physical sensing devices, results in vast amounts of sensing (measurement) data which have to be transmitted and processed in order to extract information. Data exchange and manipulation raise several concerns regarding priva\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 19: character_data_augmentation\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 55337, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"62facf055f76389607a9561e397e280cc123d468\", \"title\": \"Sharing Data by Language Family: Data Augmentation for Romance Language Morpheme Segmentation\", \"abstract\": \"This paper presents a basic character level sequence-to-sequence approach to morpheme segmentation for the following Romance languages: French, Italian, and Spanish. We experiment with adding a small set of additional linguistic features, as well as with sharing training da\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 1803, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"62facf055f76389607a9561e397e280cc123d468\", \"title\": \"Sharing Data by Language Family: Data Augmentation for Romance Language Morpheme Segmentation\", \"abstract\": \"This paper presents a basic character level sequence-to-sequence approach to morpheme segmentation for the following Romance languages: French, Italian, and Spanish. We experiment with adding a small set of additional linguistic features, as well as with sharing training dat\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 566, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"712bed095c54ff3023dec1497ca9d89c428c2133\", \"title\": \"ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations\", \"abstract\": \"Neural image classifiers can often learn to make predictions by overly relying on non-predictive features that are spuriously correlated with the class labels in the training data. This leads to poor performance in real-world atypical scenarios where such features are abse\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 5019, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"2bdc5c124ab4d7c21f9ef6f3637f02155de52dc8\", \"title\": \"Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?\", \"abstract\": \"Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, i\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 10166, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"90022c80ea85a41d8d1a7765fd95824bf3a9830f\", \"title\": \"Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\", \"abstract\": null, \"venue\": \"Natural Language Processing and Chinese Computing\", \"year\": 2023, \"citationCount\": 13, \"citationStyles\": {\"bibtex\": \"@Article{Dong2023RevisitIP,\\n author = {Guanting Dong and Jinxu Zhao and Tingfeng Hui and Daichi Guo and Wenlong Wan and\n",
      "Decision made: novel after round 5\n",
      "\n",
      "Checking novelty of idea 20: adaptive_hyperparameters\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 8833, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"b57348cc65ab2f32c3892554a2f00be1b5f2c85d\", \"title\": \"Improving adversarial robustness of deep neural networks via adaptive margin evolution\", \"abstract\": \"Adversarial training is the most popular and general strategy to improve Deep Neural Network (DNN) robustness against adversarial noises. Many adversarial training methods have been proposed in the past few years. However, most of these methods are highly susceptible to hyperparame\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 58, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"33bdd0b21d737a483d564a3aa19827425172c339\", \"title\": \"Quantifying the Hyperparameter Sensitivity of Neural Networks for Character-level Sequence-to-Sequence Tasks\", \"abstract\": \"Hyperparameter tuning, the process of searching for suitable hyperparameters, becomes more difficult as the computing resources required to train neural networks continue to grow. This topic continues to receive little attention and discussion\\u2014much of it he\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 617, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"0654d0addc15c0c76e39ef71c48164e395c6993f\", \"title\": \"ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback\", \"abstract\": \"Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) furthe\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 21: attention_head_specialization\n",
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 3870, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"5c74671d756ef2d742b15fdca97fdc68d6cff691\", \"title\": \"Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition\", \"abstract\": \"Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture. Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on \n",
      "Decision made: not novel after round 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'adaptive_block_size',\n",
       "  'Title': 'Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training',\n",
       "  'Experiment': 'Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.',\n",
       "  'Interestingness': 6,\n",
       "  'Feasibility': 4,\n",
       "  'Novelty': 4,\n",
       "  'novel': True},\n",
       " {'Name': 'layerwise_learning_rates',\n",
       "  'Title': 'Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models',\n",
       "  'Experiment': 'Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.',\n",
       "  'Interestingness': 4,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 2,\n",
       "  'novel': True},\n",
       " {'Name': 'parameter_sharing',\n",
       "  'Title': 'Exploring Parameter Sharing in Character-Level Transformer Models',\n",
       "  'Experiment': 'Modify the Block class to optionally share parameters between some of the transformer blocks. Introduce a new configuration parameter to control the sharing size. Adjust the training loop to monitor convergence speed and final performance with shared vs non-shared configurations. Compare model performance, parameter size, and training time.',\n",
       "  'Interestingness': 7,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 5,\n",
       "  'novel': True},\n",
       " {'Name': 'knowledge_distillation',\n",
       "  'Title': 'Knowledge Distillation for Enhancing Character-Level Language Models',\n",
       "  'Experiment': \"Implement a knowledge distillation framework where a small language model (student) learns from a larger pre-trained model (teacher). Use an existing language model and adapt it to provide character-level outputs if necessary. Modify the training loop to incorporate a distillation loss, which involves minimizing the difference between the student's outputs and the teacher's soft labels. Compare the student model's performance with and without distillation in terms of accuracy, generalization, and inference speed.\",\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 6,\n",
       "  'novel': True},\n",
       " {'Name': 'curriculum_learning',\n",
       "  'Title': 'Curriculum Learning: Progressive Complexity in Character-Level Model Training',\n",
       "  'Experiment': 'Implement a curriculum learning strategy where the training data is organized by increasing complexity. Modify the data loader to start training on simpler sequences (e.g., shorter sequences or sequences with common words) and gradually introduce more complex sequences as training progresses. Track and compare the convergence speed, final performance, and robustness of the model trained with and without curriculum learning.',\n",
       "  'Interestingness': 7,\n",
       "  'Feasibility': 5,\n",
       "  'Novelty': 6,\n",
       "  'novel': True},\n",
       " {'Name': 'attention_sparsity',\n",
       "  'Title': 'Exploring Attention Sparsity in Character-Level Transformer Models',\n",
       "  'Experiment': 'Modify the CausalSelfAttention class to implement sparse attention mechanisms. Introduce a new configuration parameter to control the sparsity level. Begin by experimenting with predefined sparsity patterns such as block sparsity or strided sparsity. Quantitatively evaluate the impact on computational efficiency using GPU memory usage and inference speed, as well as on model performance using metrics like training loss and accuracy. Compare these metrics against a baseline model with dense attention. If successful, explore learnable sparsity patterns as a subsequent phase.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'adaptive_computation_time',\n",
       "  'Title': 'Adaptive Computation Time in Character-Level Language Models',\n",
       "  'Experiment': 'Integrate a simplified adaptive computation time mechanism by adding a halting unit to a few selected transformer blocks. The halting unit will decide whether further computation is needed based on the input. Adjust the training loop to include a regularization term for minimizing computation steps. Evaluate the impact on model efficiency by measuring computation time and energy consumption and ensure performance by tracking accuracy against a baseline model. Allow tuning of the regularization term to balance efficiency and accuracy.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'weight_noise_injection',\n",
       "  'Title': 'Improving Robustness and Generalization with Weight Noise Injection',\n",
       "  'Experiment': \"Modify the training loop to introduce random Gaussian noise to the model's weights after each optimization step. Implement a configuration parameter to control the noise level. Evaluate the model's performance on the validation data with and without noise injection to determine its impact on generalization. Additionally, test the model's robustness by introducing small perturbations to the input data during validation and measuring performance changes.\",\n",
       "  'Interestingness': 7,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 5,\n",
       "  'novel': False},\n",
       " {'Name': 'mixed_precision_training',\n",
       "  'Title': 'Enhancing Training Efficiency with Mixed Precision in Character-Level Language Models',\n",
       "  'Experiment': \"Implement mixed precision training using PyTorch's automatic mixed precision (AMP). Modify the training loop to enable mixed precision and compare the training time, memory usage, and model performance against the baseline full precision training. Evaluate the impact on both training and inference speed and ensure model accuracy remains consistent.\",\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 5,\n",
       "  'novel': True},\n",
       " {'Name': 'dynamic_tokenization',\n",
       "  'Title': 'Dynamic Tokenization Strategies for Character-Level Language Models',\n",
       "  'Experiment': 'Modify the data loader to implement dynamic tokenization strategies, starting with character-level granularity and shifting to subword-level or word-level granularity as training progresses. Implement a mechanism to control the transition based on training phase or performance metrics. Evaluate the impact on model performance, efficiency, and convergence speed compared to a static character-level tokenization approach.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'early_exit_strategy',\n",
       "  'Title': 'Early Exit Strategy: Efficient Inference in Character-Level Transformer Models',\n",
       "  'Experiment': 'Incorporate an early exit mechanism in the GPT model by adding a confidence score assessment within each Block. Modify the forward method to include a confidence check after each block, using a simple metric such as the maximum softmax probability to decide on early exit. Compare inference speed and accuracy with and without early exit to evaluate computational efficiency and model performance trade-offs.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'adaptive_architecture_search',\n",
       "  'Title': 'Adaptive Architecture Search for Character-Level Language Models',\n",
       "  'Experiment': 'Integrate an adaptive mechanism in the training loop to periodically evaluate model performance and adjust specific architectural components, such as the number of layers or embedding size. Trigger adjustments based on predefined criteria like validation loss plateau or epoch milestones. Compare the performance, training time, and complexity of the final adaptive architecture against a static baseline.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'memory_augmented_networks',\n",
       "  'Title': 'Enhancing Character-Level Language Models with Memory-Augmented Neural Networks',\n",
       "  'Experiment': \"Integrate a parameterizable memory buffer into the GPT model architecture. Modify the GPT class to include this memory component, which stores a fixed number of past hidden states. Allow the buffer size to be tuned according to sequence length or compute resources. Design read operations to allow the model to access this buffer during the forward pass. Adjust the training loop to update the buffer's contents at each step. Evaluate the model's performance on the validation set, focusing on tasks with long-range dependencies. Compare the performance, memory efficiency, and computational overhead with the baseline model.\",\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'adaptive_dropout',\n",
       "  'Title': 'Adaptive Dropout: Dynamic Regularization in Character-Level Transformer Models',\n",
       "  'Experiment': 'Implement an adaptive dropout mechanism where the dropout rate is adjusted dynamically during training. Modify the training loop to track validation loss or other performance metrics, and adjust the dropout rate accordingly. For example, increase the dropout rate if validation loss decreases, suggesting potential overfitting. Compare the convergence speed, final validation performance, and robustness of the model with and without adaptive dropout.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 6,\n",
       "  'novel': True},\n",
       " {'Name': 'attention_temperature',\n",
       "  'Title': 'Exploring Attention Temperature in Transformer Models',\n",
       "  'Experiment': 'Modify the CausalSelfAttention class to incorporate an attention temperature parameter. Adjust the calculation of attention scores by dividing them by the temperature. Experiment with both fixed and dynamically adjusted temperatures based on validation performance. Compare the convergence speed, final performance, and robustness of the model with different temperature settings against the baseline model.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 7,\n",
       "  'novel': False},\n",
       " {'Name': 'model_pruning',\n",
       "  'Title': 'Efficient Model Pruning in Character-Level Transformer Models',\n",
       "  'Experiment': 'Implement structured pruning within the training loop by identifying and removing less important components such as heads in multi-head attention or neurons in MLP layers. Use metrics like weight magnitude or gradient-based importance to decide which components to prune. Modify the training loop to apply pruning at specific intervals, such as every few epochs. Evaluate the impact of pruning on model size, training time, and performance metrics, including validation loss and accuracy. Compare these metrics against the baseline model without pruning. Experiment with different pruning ratios and strategies to identify the optimal trade-off between model compactness and performance, ensuring the model maintains sufficient accuracy for practical use.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 7,\n",
       "  'novel': False},\n",
       " {'Name': 'multi_task_learning',\n",
       "  'Title': 'Multi-Task Learning in Character-Level Language Models',\n",
       "  'Experiment': 'Modify the data loader to generate labels for auxiliary tasks such as word boundary prediction. Adjust the GPT model to include additional output layers for these tasks. Update the training loop to compute auxiliary losses and integrate them with the main loss. Carefully select auxiliary tasks that are directly related to the main task and measure their performance. Experiment with different weightings for the auxiliary losses to evaluate their impact on the main task performance. Track metrics such as training loss, validation loss, and accuracy to compare the multi-task model against the baseline.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'elastic_computation',\n",
       "  'Title': 'Elastic Computation: Dynamic Layer Utilization in Language Models',\n",
       "  'Experiment': 'Implement a mechanism in the model to dynamically adjust the number of active layers based on input complexity. Modify the forward method to include a decision-making module that evaluates input complexity and adjusts the number of transformer blocks used. Compare the efficiency (e.g., computation time, energy consumption) and performance (e.g., accuracy, loss) against a static model configuration.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 6,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'data_noise_robustness',\n",
       "  'Title': 'Enhancing Model Robustness Against Data Corruption: A Noise Injection Approach',\n",
       "  'Experiment': \"Modify the data loader to introduce controlled noise to the input data during training. Implement a configuration parameter to control the noise level and type (e.g., Gaussian, salt-and-pepper). Conduct experiments to find the optimal noise level by evaluating the model's performance on both noisy and clean validation sets. Track metrics such as validation loss and accuracy to compare the noisily trained model against a baseline trained on clean data, ensuring that robustness does not compromise performance on clean data.\",\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 6,\n",
       "  'novel': True},\n",
       " {'Name': 'character_data_augmentation',\n",
       "  'Title': 'Character-Level Data Augmentation for Robust Language Model Training',\n",
       "  'Experiment': 'Modify the data loader to implement character-level data augmentation strategies, such as random deletion, swapping, or insertion of characters. Introduce configuration parameters to control the augmentation probability and type. Evaluate the impact on model generalization by comparing the validation loss and accuracy against a baseline model trained without augmentation. Additionally, assess robustness by introducing noise to the validation set and comparing performance.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'adaptive_hyperparameters',\n",
       "  'Title': 'Adaptive Hyperparameter Tuning for Enhanced Learning in Character-Level Models',\n",
       "  'Experiment': 'Modify the training loop to dynamically adjust hyperparameters like learning rate and dropout based on performance metrics during training. Implement a feedback mechanism using simple heuristics or automated methods (e.g., Bayesian optimization) to tune these hyperparameters in response to changes in training or validation loss. Compare model performance and adaptability with a baseline using static hyperparameters, focusing on metrics like convergence speed and final accuracy.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'attention_head_specialization',\n",
       "  'Title': 'Attention Head Specialization: Encouraging Diverse Information Processing in Transformer Models',\n",
       "  'Experiment': 'Modify the CausalSelfAttention class to include a regularization term that penalizes similar attention patterns across heads using cosine similarity. Implement visualization tools to analyze attention patterns of each head. Evaluate model performance, including accuracy and inference speed, with and without head specialization. Compare attention pattern diversity and task-specific improvements.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': False}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(osp.join(BASE_DIR, \"ideas.json\"), \"r\") as f:\n",
    "    ideas = json.load(f)\n",
    "check_idea_novelty(ideas, BASE_DIR, client, model, max_num_iterations=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
