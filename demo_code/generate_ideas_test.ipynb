{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "from typing import List, Dict, Union\n",
    "from llm import get_response_from_llm, extract_json_between_markers\n",
    "import openai\n",
    "import requests\n",
    "import backoff\n",
    "\n",
    "from prompt import idea_first_prompt, idea_reflection_prompt, novelty_prompt, novelty_system_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_API_KEY = os.getenv(\"S2_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ideas(\n",
    "    base_dir,\n",
    "    client,\n",
    "    model,\n",
    "    skip_generation=False,\n",
    "    max_num_generations=20,\n",
    "    num_reflections=5,\n",
    "):\n",
    "    # 如果 skip_generation 为真，则跳过生成过程并从文件中加载现有的想法\n",
    "    if skip_generation:\n",
    "        try:\n",
    "            with open(osp.join(base_dir, \"ideas.json\"), \"r\") as f:\n",
    "                ideas = json.load(f)\n",
    "            print(\"Loaded existing ideas:\")\n",
    "            for idea in ideas:\n",
    "                print(idea)\n",
    "            return ideas  # 返回从文件中加载的想法\n",
    "        except FileNotFoundError:\n",
    "            print(\"No existing ideas found. Generating new ideas.\")  # 文件不存在\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding existing ideas. Generating new ideas.\")  # JSON 解码错误\n",
    "\n",
    "    # 初始化一个存储想法的列表\n",
    "    idea_str_archive = []\n",
    "    \n",
    "    # 从文件中加载种子想法并将其转换为字符串格式\n",
    "    with open(osp.join(base_dir, \"seed_ideas.json\"), \"r\") as f:\n",
    "        seed_ideas = json.load(f)\n",
    "    for seed_idea in seed_ideas:\n",
    "        idea_str_archive.append(json.dumps(seed_idea))\n",
    "\n",
    "    # 读取包含实验代码的文件内容\n",
    "    with open(osp.join(base_dir, \"experiment.py\"), \"r\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    # 读取包含提示信息的文件内容\n",
    "    with open(osp.join(base_dir, \"prompt.json\"), \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # 提取系统提示\n",
    "    idea_system_prompt = prompt[\"system\"]\n",
    "\n",
    "    # 开始生成想法，最多生成 max_num_generations 次\n",
    "    for _ in range(max_num_generations):\n",
    "        print()\n",
    "        print(f\"Generating idea {_ + 1}/{max_num_generations}\")\n",
    "        try:\n",
    "            # 将之前生成的想法转化为字符串\n",
    "            prev_ideas_string = \"\\n\\n\".join(idea_str_archive)\n",
    "\n",
    "            # 消息历史初始化为空\n",
    "            msg_history = []\n",
    "            print(f\"Iteration 1/{num_reflections}\")\n",
    "            \n",
    "            # 使用 LLM 生成新的想法\n",
    "            text, msg_history = get_response_from_llm(\n",
    "                idea_first_prompt.format(\n",
    "                    task_description=prompt[\"task_description\"],\n",
    "                    code=code,\n",
    "                    prev_ideas_string=prev_ideas_string,\n",
    "                    num_reflections=num_reflections,\n",
    "                ),\n",
    "                client=client,\n",
    "                model=model,\n",
    "                system_message=idea_system_prompt,\n",
    "                msg_history=msg_history,\n",
    "            )\n",
    "            \n",
    "            # 解析输出，尝试从中提取 JSON 数据\n",
    "            json_output = extract_json_between_markers(text)\n",
    "            assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
    "            print(json_output)\n",
    "\n",
    "            # 如果反思次数大于1，则进行多次迭代改进\n",
    "            if num_reflections > 1:\n",
    "                for j in range(num_reflections - 1):\n",
    "                    print(f\"Iteration {j + 2}/{num_reflections}\")\n",
    "                    text, msg_history = get_response_from_llm(\n",
    "                        idea_reflection_prompt.format(\n",
    "                            current_round=j + 2, num_reflections=num_reflections\n",
    "                        ),\n",
    "                        client=client,\n",
    "                        model=model,\n",
    "                        system_message=idea_system_prompt,\n",
    "                        msg_history=msg_history,\n",
    "                    )\n",
    "                    # 再次解析输出，尝试从中提取 JSON 数据\n",
    "                    json_output = extract_json_between_markers(text)\n",
    "                    assert (\n",
    "                        json_output is not None\n",
    "                    ), \"Failed to extract JSON from LLM output\"\n",
    "                    print(json_output)\n",
    "\n",
    "                    # 如果输出中包含 \"I am done\" 字样，则认为已收敛，提前退出循环\n",
    "                    if \"I am done\" in text:\n",
    "                        print(f\"Idea generation converged after {j + 2} iterations.\")\n",
    "                        break\n",
    "\n",
    "            # 将新生成的想法加入存档\n",
    "            idea_str_archive.append(json.dumps(json_output))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate idea: {e}\")  # 捕获异常并打印错误信息\n",
    "            continue\n",
    "\n",
    "    # 保存生成的所有想法到文件\n",
    "    ideas = []\n",
    "    for idea_str in idea_str_archive:\n",
    "        ideas.append(json.loads(idea_str))\n",
    "\n",
    "    with open(osp.join(base_dir, \"ideas.json\"), \"w\") as f:\n",
    "        json.dump(ideas, f, indent=4)\n",
    "\n",
    "    return ideas  # 返回生成的想法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_idea(\n",
    "    base_dir,\n",
    "    client,\n",
    "    model,\n",
    "    prev_idea_archive=[],\n",
    "    num_reflections=5,\n",
    "    max_attempts=10,\n",
    "):\n",
    "    # 初始化想法存档，并获取当前存档的大小\n",
    "    idea_archive = prev_idea_archive\n",
    "    original_archive_size = len(idea_archive)\n",
    "\n",
    "    print(f\"Generating idea {original_archive_size + 1}\")\n",
    "\n",
    "    # 如果存档为空，则加载种子想法\n",
    "    if len(prev_idea_archive) == 0:\n",
    "        print(f\"First iteration, taking seed ideas\")\n",
    "        with open(osp.join(base_dir, \"seed_ideas.json\"), \"r\") as f:\n",
    "            seed_ideas = json.load(f)\n",
    "        for seed_idea in seed_ideas[:1]:  # 仅加载第一个种子想法\n",
    "            idea_archive.append(seed_idea)\n",
    "    else:\n",
    "        # 否则，从文件中读取实验代码和提示\n",
    "        with open(osp.join(base_dir, \"experiment.py\"), \"r\") as f:\n",
    "            code = f.read()\n",
    "        with open(osp.join(base_dir, \"prompt.json\"), \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "        idea_system_prompt = prompt[\"system\"]\n",
    "\n",
    "        # 尝试生成想法，最多进行 max_attempts 次尝试\n",
    "        for _ in range(max_attempts):\n",
    "            try:\n",
    "                # 将现有想法转换为字符串形式\n",
    "                idea_strings = []\n",
    "                for idea in idea_archive:\n",
    "                    idea_strings.append(json.dumps(idea))\n",
    "                prev_ideas_string = \"\\n\\n\".join(idea_strings)\n",
    "\n",
    "                # 初始化消息历史\n",
    "                msg_history = []\n",
    "                print(f\"Iteration 1/{num_reflections}\")\n",
    "                \n",
    "                # 使用 LLM 生成新的想法\n",
    "                text, msg_history = get_response_from_llm(\n",
    "                    idea_first_prompt.format(\n",
    "                        task_description=prompt[\"task_description\"],\n",
    "                        code=code,\n",
    "                        prev_ideas_string=prev_ideas_string,\n",
    "                        num_reflections=num_reflections,\n",
    "                    )\n",
    "                    + \"\"\"\n",
    "Completed ideas have an additional \"Score\" field which indicates the assessment by an expert ML reviewer.\n",
    "This is on a standard 1-10 ML conference scale.\n",
    "Scores of 0 indicate the idea failed either during experimentation, writeup or reviewing.\n",
    "\"\"\",\n",
    "                    client=client,\n",
    "                    model=model,\n",
    "                    system_message=idea_system_prompt,\n",
    "                    msg_history=msg_history,\n",
    "                )\n",
    "                \n",
    "                # 解析输出，尝试从中提取 JSON 数据\n",
    "                json_output = extract_json_between_markers(text)\n",
    "                assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
    "                print(json_output)\n",
    "\n",
    "                # 如果反思次数大于1，则进行多次迭代改进\n",
    "                if num_reflections > 1:\n",
    "                    for j in range(num_reflections - 1):\n",
    "                        print(f\"Iteration {j + 2}/{num_reflections}\")\n",
    "                        text, msg_history = get_response_from_llm(\n",
    "                            idea_reflection_prompt.format(\n",
    "                                current_round=j + 2, num_reflections=num_reflections\n",
    "                            ),\n",
    "                            client=client,\n",
    "                            model=model,\n",
    "                            system_message=idea_system_prompt,\n",
    "                            msg_history=msg_history,\n",
    "                        )\n",
    "                        \n",
    "                        # 再次解析输出，尝试从中提取 JSON 数据\n",
    "                        json_output = extract_json_between_markers(text)\n",
    "                        assert (\n",
    "                            json_output is not None\n",
    "                        ), \"Failed to extract JSON from LLM output\"\n",
    "                        print(json_output)\n",
    "\n",
    "                        # 如果输出中包含 \"I am done\" 字样，则认为已收敛，提前退出循环\n",
    "                        if \"I am done\" in text:\n",
    "                            print(\n",
    "                                f\"Idea generation converged after {j + 2} iterations.\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                # 将新生成的想法加入存档\n",
    "                idea_archive.append(json_output)\n",
    "                break  # 成功生成想法，退出尝试循环\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate idea: {e}\")  # 捕获异常并打印错误信息\n",
    "                continue\n",
    "\n",
    "    # 保存生成的所有想法到文件\n",
    "    with open(osp.join(base_dir, \"ideas.json\"), \"w\") as f:\n",
    "        json.dump(idea_archive, f, indent=4)\n",
    "\n",
    "    return idea_archive  # 返回更新后的想法存档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_backoff(details):\n",
    "    print(\n",
    "        f\"Backing off {details['wait']:0.1f} seconds after {details['tries']} tries \"\n",
    "        f\"calling function {details['target'].__name__} at {time.strftime('%X')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating idea 1/20\n",
      "Iteration 1/5\n",
      "{'Name': 'attention_patterns_grokking', 'Title': 'Attention Patterns During Grokking: Analyzing the Evolution of Attention in Transformer Models', 'Experiment': 'Implement a mechanism to log and visualize attention scores from the self-attention layers of the Transformer at various stages during training. Analyze these scores to identify any shifts in attention patterns that coincide with the onset of grokking. This could involve adding hooks to the self-attention layers in the `DecoderBlock` class and periodically storing the attention matrices. Compare these patterns before, during, and after grokking to discern any notable changes.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'attention_patterns_grokking', 'Title': 'Attention Patterns During Grokking: Analyzing the Evolution of Attention in Transformer Models', 'Experiment': \"Implement hooks in the self-attention layers of the `DecoderBlock` class to log attention scores at regular intervals during training (e.g., every 1000 steps). Use these logs to visualize and analyze changes in attention distributions over time. Apply clustering techniques or statistical methods to compare attention patterns before, during, and after grokking. This analysis should aim to identify any shifts in focus that correlate with the model's transition to perfect generalization.\", 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'attention_patterns_grokking', 'Title': 'Attention Patterns During Grokking: Analyzing the Evolution of Attention in Transformer Models', 'Experiment': \"Implement hooks in the self-attention layers of the `DecoderBlock` class to log attention scores at regular intervals during training (e.g., every 1000 steps). Use these logs to visualize and analyze changes in attention distributions over time. Apply clustering techniques or statistical methods to compare attention patterns before, during, and after grokking. This analysis should aim to identify any shifts in focus that correlate with the model's transition to perfect generalization.\", 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 2/20\n",
      "Iteration 1/5\n",
      "{'Name': 'optimization_strategy_grokking', 'Title': 'Optimization Strategy and Grokking: Exploring the Role of Optimizers and Learning Rates', 'Experiment': 'Implement and compare the effects of different optimizers (e.g., SGD, RMSprop, Adam) and learning rate schedules (e.g., step decay, cosine annealing) on the grokking phenomenon. Modify the run function to include these variations and log the time it takes for models to achieve perfect generalization. Analyze how different strategies affect the convergence speed and stability of grokking across datasets.', 'Interestingness': 7, 'Feasibility': 7, 'Novelty': 6}\n",
      "Iteration 2/5\n",
      "{'Name': 'optimization_strategy_grokking', 'Title': 'Optimization Strategy and Grokking: Exploring the Role of Optimizers and Learning Rates', 'Experiment': 'Implement and compare the effects of different optimizers (e.g., SGD, RMSprop, Adam) and learning rate schedules (e.g., step decay, cosine annealing) on the grokking phenomenon. Modify the run function to include these variations and log the time it takes for models to achieve perfect generalization. Analyze how different strategies affect the convergence speed and stability of grokking across datasets.', 'Interestingness': 7, 'Feasibility': 7, 'Novelty': 6}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 3/20\n",
      "Iteration 1/5\n",
      "{'Name': 'representation_dynamics_grokking', 'Title': 'Representation Dynamics and Grokking: Analyzing the Evolution of Feature Representations in Transformer Models', 'Experiment': 'Implement hooks in the Transformer model to extract and log the hidden activations of each layer at regular intervals during training. Use dimensionality reduction techniques such as PCA or t-SNE to visualize these high-dimensional activations and track their evolution over time. Analyze the trajectories of these representations before, during, and after grokking to identify any patterns or shifts associated with the transition to perfect generalization. This could involve studying the clustering of representations and their separability across different classes.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'representation_dynamics_grokking', 'Title': 'Representation Dynamics and Grokking: Analyzing the Evolution of Feature Representations in Transformer Models', 'Experiment': 'Implement hooks in the Transformer model to extract and log hidden activations from the first and last layers at regular intervals during training. Apply dimensionality reduction techniques such as PCA or t-SNE to visualize these activations and track their evolution. Focus the analysis on changes in clustering and separability of these representations across different classes before, during, and after grokking, aiming to identify patterns associated with the transition to perfect generalization.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'representation_dynamics_grokking', 'Title': 'Representation Dynamics and Grokking: Analyzing the Evolution of Feature Representations in Transformer Models', 'Experiment': 'Implement hooks in the Transformer model to extract and log hidden activations from the first and last layers at regular intervals, such as every 1000 training steps. Use dimensionality reduction techniques like PCA or t-SNE to visualize these activations, focusing on changes in clustering and separability across different classes. This analysis should be conducted before, during, and after grokking, aiming to identify representation patterns associated with the transition to perfect generalization. Visualizations will be used for both qualitative insights and quantitative measures like cluster compactness.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 4/5\n",
      "{'Name': 'representation_dynamics_grokking', 'Title': 'Representation Dynamics and Grokking: Analyzing the Evolution of Feature Representations in Transformer Models', 'Experiment': 'Implement hooks in the Transformer model to extract and log hidden activations from the first and last layers at regular intervals, such as every 1000 training steps. Use dimensionality reduction techniques like PCA or t-SNE to visualize these activations, focusing on changes in clustering and separability across different classes. This analysis should be conducted before, during, and after grokking, aiming to identify representation patterns associated with the transition to perfect generalization. Visualizations will be used for both qualitative insights and quantitative measures like cluster compactness.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 4/20\n",
      "Iteration 1/5\n",
      "{'Name': 'architecture_complexity_grokking', 'Title': 'Architecture Complexity and Grokking: Exploring the Role of Model Depth and Width', 'Experiment': 'Implement experiments to vary the number of layers and attention heads in the Transformer model. Modify the initialization parameters in the Transformer class to allow for easy adjustment of these architectural features. Run systematic experiments with different combinations of depth and width, measuring the time it takes for models to achieve perfect generalization and analyzing the relationship between architectural complexity and grokking. Use the final training and validation metrics to assess how these changes affect performance and generalization.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'architecture_complexity_grokking', 'Title': 'Architecture Complexity and Grokking: Exploring the Role of Model Depth and Width', 'Experiment': 'Implement experiments to vary the number of layers and attention heads in the Transformer model. Modify the initialization parameters in the Transformer class to allow for easy adjustment of these architectural features. Run systematic experiments with different combinations of depth and width, measuring the time it takes for models to achieve perfect generalization and analyzing the relationship between architectural complexity and grokking. Use the final training and validation metrics to assess how these changes affect performance and generalization.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 5/20\n",
      "Iteration 1/5\n",
      "{'Name': 'noise_impact_grokking', 'Title': 'Noise Impact on Grokking: Assessing the Role of Noise in Neural Network Generalization', 'Experiment': 'Incorporate noise injection into the training process by modifying the train function. Implement options to add noise to input data or hidden layers, with adjustable noise levels and types (e.g., Gaussian, uniform). Conduct experiments with varying noise parameters to measure their effect on the grokking timeline and final generalization performance. Use metrics such as the number of training steps to achieve 99% validation accuracy and final validation loss to assess the impact.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'noise_impact_grokking', 'Title': 'Noise Impact on Grokking: Assessing the Role of Noise in Neural Network Generalization', 'Experiment': 'Incorporate noise injection into the training process by modifying the train function. Implement options to add Gaussian noise to input data or hidden layers, with adjustable noise levels. Conduct experiments with varying noise levels to measure their effect on the grokking timeline and final generalization performance. Use metrics such as the number of training steps to achieve 99% validation accuracy and final validation loss to assess the impact.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'noise_impact_grokking', 'Title': 'Noise Impact on Grokking: Assessing the Role of Noise in Neural Network Generalization', 'Experiment': 'Incorporate noise injection into the training process by modifying the train function. Implement options to add Gaussian noise to input data or hidden layers, with adjustable noise levels. Introduce noise at specific intervals during training to observe its impact at different training stages. Conduct experiments with varying noise levels to measure their effect on the grokking timeline and final generalization performance. Use metrics such as the number of training steps to achieve 99% validation accuracy and final validation loss to assess the impact.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 4/5\n",
      "{'Name': 'noise_impact_grokking', 'Title': 'Noise Impact on Grokking: Assessing the Role of Noise in Neural Network Generalization', 'Experiment': 'Incorporate noise injection into the training process by modifying the train function. Implement options to add Gaussian noise to input data or hidden layers, with adjustable noise levels. Introduce noise at specific intervals during training to observe its impact at different training stages. Conduct experiments with varying noise levels to measure their effect on the grokking timeline and final generalization performance. Use metrics such as the number of training steps to achieve 99% validation accuracy and final validation loss to assess the impact.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 4 iterations.\n",
      "\n",
      "Generating idea 6/20\n",
      "Iteration 1/5\n",
      "{'Name': 'weight_sparsity_grokking', 'Title': 'Weight Sparsity and Grokking: Exploring the Impact of Pruning on Generalization', 'Experiment': 'Implement a pruning mechanism within the training loop to periodically remove a fraction of the least significant weights based on magnitude. Modify the train function to include pruning steps after certain training intervals. Analyze the number of training steps required to reach 99% validation accuracy and final validation performance across different pruning strategies. This will help assess how pruning impacts the grokking timeline and generalization ability.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'weight_sparsity_grokking', 'Title': 'Weight Sparsity and Grokking: Exploring the Impact of Pruning on Generalization', 'Experiment': 'Implement a pruning mechanism within the training loop to periodically remove a fraction of the least significant weights based on magnitude. Modify the train function to include pruning steps after certain training intervals. Analyze the number of training steps required to reach 99% validation accuracy and final validation performance across different pruning strategies. This will help assess how pruning impacts the grokking timeline and generalization ability.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 7/20\n",
      "Iteration 1/5\n",
      "{'Name': 'data_augmentation_grokking', 'Title': 'Data Augmentation and Grokking: Exploring the Impact of Input Variability on Generalization', 'Experiment': 'Implement data augmentation techniques specific to each dataset type (e.g., ModSum, ModSubtract, ModDivision, Permutation). Modify the data loading pipeline in the `GroupDataset` class to include these augmentations during training. Ensure that the validation data remains unchanged for clear comparison. Conduct experiments to measure the effect of data augmentation on the number of training steps required to achieve 99% validation accuracy and on the final validation loss, comparing these metrics with baseline results without augmentation.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7}\n",
      "Iteration 2/5\n",
      "{'Name': 'data_augmentation_grokking', 'Title': 'Data Augmentation and Grokking: Exploring the Impact of Input Variability on Generalization', 'Experiment': 'Implement data augmentation techniques specific to each dataset type: for ModSum and ModSubtract, apply operand swapping (e.g., a+b = b+a); for ModDivision, introduce multiplicative inverses; for Permutation, use input permutation. Modify the data loading pipeline in the `GroupDataset` class to include these augmentations during training. Ensure that the validation data remains unchanged for clear comparison. Conduct experiments to measure the effect of data augmentation on the number of training steps required to achieve 99% validation accuracy and on the final validation loss, comparing these metrics with baseline results without augmentation.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7}\n",
      "Iteration 3/5\n",
      "{'Name': 'data_augmentation_grokking', 'Title': 'Data Augmentation and Grokking: Exploring the Impact of Input Variability on Generalization', 'Experiment': 'Implement data augmentation techniques specific to each dataset type: for ModSum and ModSubtract, apply operand swapping (e.g., a+b = b+a); for ModDivision, introduce multiplicative inverses; for Permutation, use input permutation. Modify the data loading pipeline in the `GroupDataset` class to include these augmentations during training. Ensure that the validation data remains unchanged for clear comparison. Conduct experiments to measure the effect of data augmentation on the number of training steps required to achieve 99% validation accuracy and on the final validation loss, comparing these metrics with baseline results without augmentation.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 8/20\n",
      "Iteration 1/5\n",
      "{'Name': 'curriculum_learning_grokking', 'Title': 'Curriculum Learning and Grokking: The Impact of Progressive Training Difficulty', 'Experiment': 'Modify the data loading pipeline to implement a curriculum learning strategy. Create a mechanism to rank the difficulty of training examples, starting with simpler ones and gradually progressing to more complex ones. This can involve starting with smaller numbers for mathematical operations and slowly increasing their range as training progresses. Implement this progression in the `GroupDataset` class, and adjust the training loop to handle the dynamic data selection. Measure the time taken to achieve 99% validation accuracy and compare it to baseline results to determine the impact of curriculum learning on grokking.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'curriculum_learning_grokking', 'Title': 'Curriculum Learning and Grokking: The Impact of Progressive Training Difficulty', 'Experiment': 'Modify the data loading pipeline to implement a curriculum learning strategy. Create a mechanism to rank the difficulty of training examples, starting with simpler ones and gradually progressing to more complex ones. This can involve starting with smaller numbers for mathematical operations and slowly increasing their range as training progresses. Implement this progression in the `GroupDataset` class, and adjust the training loop to handle the dynamic data selection. Measure the time taken to achieve 99% validation accuracy and compare it to baseline results to determine the impact of curriculum learning on grokking.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 9/20\n",
      "Iteration 1/5\n",
      "{'Name': 'representation_phase_grokking', 'Title': 'Representation Phase Transition and Grokking: Analyzing Hidden State Dynamics', 'Experiment': \"Implement hooks in the Transformer model to capture activations from various layers at regular intervals during training. Use dimensionality reduction techniques like PCA or t-SNE to visualize these activations, focusing on temporal changes in their structure. Analyze the evolution of these representations to identify phases or transformations associated with the model's transition from memorization to generalization. This will involve modifications to the Transformer class to log activations and the addition of scripts for visualization and analysis.\", 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'representation_phase_grokking', 'Title': 'Representation Phase Transition and Grokking: Analyzing Hidden State Dynamics', 'Experiment': 'Modify the Transformer model to include hooks that capture activations from the beginning, middle, and end layers at predefined training intervals (e.g., every 1000 steps). Use PCA or t-SNE for dimensionality reduction of captured activations, and visualize them to observe structural changes over time. Implement scripts to automate the logging and visualization process, focusing on identifying distinctive patterns or transformations in the hidden states as the model transitions towards perfect generalization.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'representation_phase_grokking', 'Title': 'Representation Phase Transition and Grokking: Analyzing Hidden State Dynamics', 'Experiment': 'Modify the Transformer model to include hooks that capture activations from the beginning, middle, and end layers at predefined training intervals (e.g., every 1000 steps). Use PCA or t-SNE for dimensionality reduction of captured activations, and visualize them to observe structural changes over time. Implement scripts to automate the logging and visualization process, focusing on identifying distinctive patterns or transformations in the hidden states as the model transitions towards perfect generalization.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 10/20\n",
      "Iteration 1/5\n",
      "{'Name': 'initialization_grokking', 'Title': 'Initialization and Grokking: Exploring the Influence of Initial Weights on Learning Dynamics', 'Experiment': 'Modify the Transformer model initialization in the __init__ method to allow for different initialization strategies such as Xavier, He, and random Gaussian with varying variances. Conduct experiments to observe how these different initializations affect the number of steps required to achieve perfect generalization and the learning trajectory. Use metrics like training speed, final validation accuracy, and loss curves to assess the impact. Compare results against a baseline with default initialization.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'initialization_grokking', 'Title': 'Initialization and Grokking: Exploring the Influence of Initial Weights on Learning Dynamics', 'Experiment': 'Modify the Transformer model initialization in the __init__ method to allow for different initialization strategies such as Xavier, He, and random Gaussian with varying variances. Conduct experiments to observe how these different initializations affect the number of steps required to achieve perfect generalization and the learning trajectory. Use metrics like training speed, final validation accuracy, and loss curves to assess the impact. Compare results against a baseline with default initialization.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 11/20\n",
      "Iteration 1/5\n",
      "{'Name': 'interpretability_grokking', 'Title': 'Interpretability and Grokking: Analyzing Feature Importance During Generalization', 'Experiment': 'Implement interpretability methods such as feature importance or layer-wise relevance propagation (LRP) in the Transformer model. Modify the training loop to periodically compute and log these interpretability metrics at regular intervals, such as every 1000 steps. Use visualizations to analyze how the importance of different input features changes over time, particularly focusing on the period when the model transitions from overfitting to generalization. Compare the feature importance patterns across different datasets and analyze if common features emerge that signify grokking.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'interpretability_grokking', 'Title': 'Interpretability and Grokking: Analyzing Attention Weight Dynamics During Generalization', 'Experiment': 'Implement functionality to extract and log attention weights from the Transformer model at regular intervals, such as every 1000 steps. Modify the training loop to compute these weights and analyze how their distribution changes over time, particularly during the transition from overfitting to generalization. Use visualizations to illustrate shifts in attention focus and compare patterns across different datasets, aiming to identify common attention features that signify grokking.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'interpretability_grokking', 'Title': 'Interpretability and Grokking: Analyzing Attention Weight Dynamics During Generalization', 'Experiment': 'Implement functionality to extract and log attention weights from the Transformer model at regular intervals, such as every 1000 steps. Modify the training loop to compute these weights and analyze how their distribution changes over time, particularly during the transition from overfitting to generalization. Use visualizations to illustrate shifts in attention focus and compare patterns across different datasets, aiming to identify common attention features that signify grokking.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 12/20\n",
      "Iteration 1/5\n",
      "{'Name': 'layer_interaction_grokking', 'Title': 'Layer Interaction and Grokking: Investigating Inter-Layer Communication in Neural Networks', 'Experiment': 'Introduce skip connections or layer attention mechanisms in the Transformer model to assess the impact of direct inter-layer communication on grokking. Implement hooks to track the flow of information through these connections during training. Analyze how these modifications alter the learning trajectory and assess the number of steps required to achieve perfect generalization. Evaluate the final validation accuracy and loss to determine the influence of inter-layer interactions on the grokking process.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'layer_interaction_grokking', 'Title': 'Layer Interaction and Grokking: Investigating Inter-Layer Communication in Neural Networks', 'Experiment': 'Implement hooks to log layer outputs at regular intervals and analyze correlations between these layer outputs during training. Focus on determining how different layers influence the learning process and contribute to grokking. Measure changes in these correlations as the model transitions from overfitting to generalization. Evaluate how this inter-layer communication correlates with performance metrics such as validation accuracy and loss.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8}\n",
      "Iteration 3/5\n",
      "{'Name': 'layer_interaction_grokking', 'Title': 'Layer Interaction and Grokking: Investigating Inter-Layer Communication in Neural Networks', 'Experiment': 'Implement hooks to log layer outputs at regular intervals and analyze correlations between these layer outputs during training. Focus on determining how different layers influence the learning process and contribute to grokking. Measure changes in these correlations as the model transitions from overfitting to generalization. Evaluate how this inter-layer communication correlates with performance metrics such as validation accuracy and loss.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 13/20\n",
      "Iteration 1/5\n",
      "{'Name': 'negative_examples_grokking', 'Title': 'Negative Examples and Grokking: Exploring the Role of Incorrect Data in Generalization', 'Experiment': 'Modify the fetch_train_example method in the dataset classes to introduce a configurable fraction of negative examples, where the model is presented with incorrect outputs for given inputs. Implement a mechanism to control the ratio of negative to positive examples during training. Conduct experiments with varying ratios, measuring their impact on the number of training steps to achieve 99% validation accuracy and the final validation loss. Compare these metrics to baseline results to assess how negative examples influence grokking.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'negative_examples_grokking', 'Title': 'Negative Examples and Grokking: Exploring the Role of Incorrect Data in Generalization', 'Experiment': 'Modify the fetch_train_example method in the dataset classes to introduce a configurable fraction of negative examples, where the model is presented with incorrect outputs for given inputs. Implement a mechanism to control the ratio of negative to positive examples during training. Conduct experiments with varying ratios, measuring their impact on the number of training steps to achieve 99% validation accuracy and the final validation loss. Compare these metrics to baseline results to assess how negative examples influence grokking.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 14/20\n",
      "Iteration 1/5\n",
      "{'Name': 'latent_space_traversal', 'Title': 'Latent Space Traversal and Grokking: Exploring Parameter Perturbations and Generalization', 'Experiment': \"Implement a mechanism in the train function to periodically apply small Gaussian noise to the model's parameters or interpolate between different parameter states during training. This can be done every few epochs or after a fixed number of training steps. Analyze how these perturbations affect the model's journey towards perfect generalization by monitoring validation accuracy and loss. Compare with baseline runs to identify perturbations that stabilize or accelerate grokking.\", 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'latent_space_traversal', 'Title': 'Latent Space Traversal and Grokking: Exploring Parameter Perturbations and Generalization', 'Experiment': \"Modify the train function to periodically apply small, controlled Gaussian noise to the model's parameters or interpolate between parameter states during training, ensuring perturbations are subtle. Implement visualization techniques to track changes in the latent space over time, and analyze their effect on the model's path towards perfect generalization by monitoring validation accuracy and loss. Compare with baseline runs to identify perturbation patterns that stabilize or accelerate grokking.\", 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 3/5\n",
      "{'Name': 'latent_space_traversal', 'Title': 'Latent Space Traversal and Grokking: Exploring Parameter Perturbations and Generalization', 'Experiment': \"Modify the train function to periodically apply small, controlled Gaussian noise to the model's parameters or interpolate between parameter states during training, ensuring perturbations are subtle. Implement visualization techniques to track changes in the latent space over time, and analyze their effect on the model's path towards perfect generalization by monitoring validation accuracy and loss. Compare with baseline runs to identify perturbation patterns that stabilize or accelerate grokking.\", 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 15/20\n",
      "Iteration 1/5\n",
      "{'Name': 'interpolation_dynamics_grokking', 'Title': 'Interpolation Dynamics and Grokking: Analyzing Hidden Representation Evolution', 'Experiment': 'Modify the Transformer model to include hooks that capture hidden activations at regular intervals, such as every 1000 steps. Implement a mechanism to compute cosine similarities between hidden layer activations for a fixed set of input pairs, logging these metrics over time. Use these logs to analyze trends and patterns that emerge during the transition from overfitting to grokking, aiming to identify shifts in representation interpolation dynamics that correlate with improved generalization.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'interpolation_dynamics_grokking', 'Title': 'Interpolation Dynamics and Grokking: Analyzing Hidden Representation Evolution', 'Experiment': 'Modify the Transformer model to include hooks that capture hidden activations at regular intervals, such as every 1000 steps. Implement a mechanism to compute cosine similarities between hidden layer activations for a diverse and representative set of input pairs, logging these metrics over time. Use these logs to analyze trends and patterns that emerge during the transition from overfitting to grokking, aiming to identify shifts in representation interpolation dynamics that correlate with improved generalization. Visualize these trends to draw clear and quantifiable insights.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 3/5\n",
      "{'Name': 'interpolation_dynamics_grokking', 'Title': 'Interpolation Dynamics and Grokking: Analyzing Hidden Representation Evolution', 'Experiment': 'Modify the Transformer model to include hooks that capture hidden activations at regular intervals, such as every 1000 steps. Implement a mechanism to compute cosine similarities between hidden layer activations for a diverse and representative set of input pairs, which can be selected based on clustering or stratified sampling to ensure dataset diversity. Log these metrics over time and analyze trends that emerge during the transition from overfitting to grokking. Identify shifts in representation interpolation dynamics that correlate with improved generalization and visualize these trends for clear insights.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 4/5\n",
      "{'Name': 'interpolation_dynamics_grokking', 'Title': 'Interpolation Dynamics and Grokking: Analyzing Hidden Representation Evolution', 'Experiment': 'Modify the Transformer model to include hooks that capture hidden activations at regular intervals, such as every 1000 steps. Implement a mechanism to compute cosine similarities between hidden layer activations for a diverse and representative set of input pairs, selected through clustering or stratified sampling. Log these metrics over time and analyze trends that emerge during the transition from overfitting to grokking. Quantify shifts in representation interpolation dynamics using statistical measures and visualize these trends to provide clear insights. Discuss potential implications for enhancing model training and generalization strategies.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 5/5\n",
      "{'Name': 'interpolation_dynamics_grokking', 'Title': 'Interpolation Dynamics and Grokking: Analyzing Hidden Representation Evolution', 'Experiment': 'Modify the Transformer model to include hooks that capture hidden activations at regular intervals, such as every 1000 steps. Implement a mechanism to compute cosine similarities between hidden layer activations for a diverse and representative set of input pairs, selected through clustering or stratified sampling. Log these metrics over time and analyze trends that emerge during the transition from overfitting to grokking. Quantify shifts in representation interpolation dynamics using statistical measures and visualize these trends to provide clear insights. Discuss potential implications for enhancing model training and generalization strategies.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Idea generation converged after 5 iterations.\n",
      "\n",
      "Generating idea 16/20\n",
      "Iteration 1/5\n",
      "{'Name': 'sequence_length_grokking', 'Title': 'Sequence Length and Grokking: Investigating the Impact of Input Length on Learning Dynamics', 'Experiment': 'Modify the data encoding process to generate input sequences of varying lengths by altering the form_equation method in AbstractDataset. Adjust the Transformer model to accept variable-length sequences by modifying the position_embeddings and related components. Conduct experiments with different sequence lengths, measuring the number of training steps required to achieve 99% validation accuracy and final validation loss. Compare these metrics across sequence lengths to assess their influence on grokking.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'sequence_length_grokking', 'Title': 'Sequence Length and Grokking: Investigating the Impact of Input Length on Learning Dynamics', 'Experiment': 'Modify the data encoding process to generate input sequences of varying lengths by altering the form_equation method in AbstractDataset. Adjust the Transformer model to accept variable-length sequences by modifying the position_embeddings and related components. Conduct experiments with different sequence lengths, measuring the number of training steps required to achieve 99% validation accuracy and final validation loss. Compare these metrics across sequence lengths to assess their influence on grokking.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n",
      "\n",
      "Generating idea 17/20\n",
      "Iteration 1/5\n",
      "{'Name': 'data_complexity_grokking', 'Title': 'Data Complexity and Grokking: Investigating the Role of Problem Intricacy in Learning Dynamics', 'Experiment': 'Implement a complexity analysis function that evaluates the algebraic complexity of each dataset type (e.g., ModSum, ModSubtract, ModDivision, Permutation). Modify the run function to log complexity scores alongside training metrics. Conduct experiments across datasets and correlate complexity scores with grokking characteristics, such as the number of training steps to achieve 99% validation accuracy and the final validation loss. Analyze whether higher complexity problems correlate with delayed grokking or different learning dynamics. Use statistical measures to validate findings and provide insights on the relationship between data complexity and model generalization.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'data_complexity_grokking', 'Title': 'Data Complexity and Grokking: Investigating the Role of Problem Intricacy in Learning Dynamics', 'Experiment': 'Implement a function to measure problem size characteristics, such as the size of modulo and permutation length, for each dataset type. Modify the run function to log these complexity scores alongside training metrics. Conduct experiments across datasets and correlate problem size with grokking characteristics, such as training steps to achieve 99% validation accuracy and final validation loss. Analyze whether larger or more intricate problems correlate with delayed grokking. Use statistical measures to validate findings and provide insights on the relationship between problem size and model generalization.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9}\n",
      "Iteration 3/5\n",
      "{'Name': 'data_complexity_grokking', 'Title': 'Data Complexity and Grokking: Investigating the Role of Problem Intricacy in Learning Dynamics', 'Experiment': 'Implement a function to measure problem size characteristics, such as the size of modulo and permutation length, for each dataset type. Modify the run function to log these complexity scores alongside training metrics. Conduct experiments across datasets and correlate problem size with grokking characteristics, such as training steps to achieve 99% validation accuracy and final validation loss. Analyze whether larger or more intricate problems correlate with delayed grokking. Use statistical measures to validate findings and provide insights on the relationship between problem size and model generalization.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 18/20\n",
      "Iteration 1/5\n",
      "{'Name': 'symmetry_enhancement_grokking', 'Title': 'Symmetry Enhancement and Grokking: Leveraging Invariances for Improved Generalization', 'Experiment': \"Modify the loss function in the training loop to include a symmetry enforcement term. This term will penalize asymmetrical outputs for specific transformations known to be symmetrical in the dataset's domain (e.g., a+b and b+a in ModSumDataset). Monitor the effect of this modification on the grokking timeline by comparing the number of steps required to achieve 99% validation accuracy and final validation performance against baseline results without symmetry enhancement.\", 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'symmetry_enhancement_grokking', 'Title': 'Symmetry Enhancement and Grokking: Leveraging Invariances for Improved Generalization', 'Experiment': \"Focus initially on the ModSumDataset and modify the loss function to include a symmetry enforcement term. This term will be computed by evaluating the model's output on both original and operand-swapped inputs, and penalizing dissimilar outputs. Update the train function to incorporate this new loss component, and monitor its impact on the grokking timeline by comparing the number of steps required to achieve 99% validation accuracy and final validation performance against baseline results without symmetry enhancement.\", 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9}\n",
      "Iteration 3/5\n",
      "{'Name': 'symmetry_enhancement_grokking', 'Title': 'Symmetry Enhancement and Grokking: Leveraging Invariances for Improved Generalization', 'Experiment': \"Focus initially on the ModSumDataset and modify the loss function to include a symmetry enforcement term. This term will be computed by evaluating the model's output on both original and operand-swapped inputs, and penalizing dissimilar outputs. Update the train function to incorporate this new loss component, and monitor its impact on the grokking timeline by comparing the number of steps required to achieve 99% validation accuracy and final validation performance against baseline results without symmetry enhancement.\", 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 19/20\n",
      "Iteration 1/5\n",
      "{'Name': 'meta_learning_grokking', 'Title': 'Meta-Learning and Grokking: Leveraging Learning-to-Learn for Improved Generalization', 'Experiment': 'Implement a meta-learning loop where the model undergoes multiple training phases. Each phase involves training on a different permutation of the dataset with the model initialized using parameters from previous phases. Modify the run function to include this loop and log the steps required to reach 99% validation accuracy. Compare these metrics to baseline results to assess the impact of meta-learning on grokking.', 'Interestingness': 9, 'Feasibility': 6, 'Novelty': 9}\n",
      "Iteration 2/5\n",
      "{'Name': 'meta_learning_grokking', 'Title': 'Meta-Learning and Grokking: Leveraging Learning-to-Learn for Improved Generalization', 'Experiment': 'Implement a meta-learning loop by modifying the run function. Divide training into several phases, each using a different permutation of the dataset. Between phases, save model parameters and reinitialize the model to continue training with these parameters. Log metrics on the steps required to reach 99% validation accuracy for each phase, and compare improvements across phases to assess meta-learning impact.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Iteration 3/5\n",
      "{'Name': 'meta_learning_grokking', 'Title': 'Meta-Learning and Grokking: Leveraging Learning-to-Learn for Improved Generalization', 'Experiment': 'Implement a meta-learning loop by modifying the run function. Divide training into several phases, each using a different permutation of the dataset. Between phases, save model parameters and reinitialize the model to continue training with these parameters. Log metrics on the steps required to reach 99% validation accuracy for each phase, and compare improvements across phases to assess meta-learning impact.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 9}\n",
      "Idea generation converged after 3 iterations.\n",
      "\n",
      "Generating idea 20/20\n",
      "Iteration 1/5\n",
      "{'Name': 'invariant_transformations_grokking', 'Title': 'Invariant Transformations and Grokking: Exploring Semantically Stable Data Augmentation', 'Experiment': 'Implement a transformation generator that creates rule-based transformations specific to each dataset type, such as operand swapping or scaling for arithmetic operations that maintain the result modulo p. Modify the data pipeline to apply these transformations during training, ensuring the validation set remains unchanged for consistency in evaluation. Run experiments to measure the impact on the number of training steps required to achieve 99% validation accuracy and final validation loss, comparing these results to baseline runs without transformations.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Iteration 2/5\n",
      "{'Name': 'invariant_transformations_grokking', 'Title': 'Invariant Transformations and Grokking: Exploring Semantically Stable Data Augmentation', 'Experiment': 'Implement a transformation generator that creates rule-based transformations specific to each dataset type, such as operand swapping or scaling for arithmetic operations that maintain the result modulo p. Modify the data pipeline to apply these transformations during training, ensuring the validation set remains unchanged for consistency in evaluation. Run experiments to measure the impact on the number of training steps required to achieve 99% validation accuracy and final validation loss, comparing these results to baseline runs without transformations.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8}\n",
      "Idea generation converged after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"./template_data/grokking/\"\n",
    "client = openai.OpenAI()\n",
    "model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "ideas_generation = generate_ideas(base_dir=BASE_DIR, client=client, model=model, skip_generation=False, max_num_generations=20, num_reflections=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo, requests.exceptions.HTTPError, on_backoff=on_backoff\n",
    ")\n",
    "def search_for_papers(query, result_limit=10) -> Union[None, List[Dict]]:\n",
    "    # 检查查询字符串是否为空\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    # 发起 HTTP GET 请求，查询论文数据\n",
    "    rsp = requests.get(\n",
    "        \"https://api.semanticscholar.org/graph/v1/paper/search\",\n",
    "        headers={\"X-API-KEY\": S2_API_KEY},\n",
    "        params={\n",
    "            \"query\": query,\n",
    "            \"limit\": result_limit,\n",
    "            \"fields\": \"title,authors,venue,year,abstract,citationStyles,citationCount\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # 打印响应状态码\n",
    "    # print(f\"Response Status Code: {rsp.status_code}\")\n",
    "    \n",
    "    # 打印响应内容的前500个字符\n",
    "    # print(f\"Response Content: {rsp.text[:500]}\")\n",
    "    \n",
    "    # 如果响应状态码不是200，则引发HTTPError异常\n",
    "    rsp.raise_for_status()\n",
    "    \n",
    "    # 解析响应的 JSON 数据\n",
    "    results = rsp.json()\n",
    "    \n",
    "    # 获取总结果数\n",
    "    total = results[\"total\"]\n",
    "    \n",
    "    # 等待1秒钟，以防止频繁请求\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    # 如果没有结果，返回 None\n",
    "    if not total:\n",
    "        return None\n",
    "    \n",
    "    # 获取论文数据列表\n",
    "    papers = results[\"data\"]\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_idea_novelty(\n",
    "    ideas,  # 需要检查的想法列表\n",
    "    base_dir,  # 存放实验代码和提示的基础目录\n",
    "    client,  # 用于与LLM交互的客户端对象\n",
    "    model,  # 用于执行查询的LLM模型名称\n",
    "    max_num_iterations=10,  # 最大迭代次数，默认值为10\n",
    "):\n",
    "    # 读取实验代码文件 experiment.py\n",
    "    with open(osp.join(base_dir, \"experiment.py\"), \"r\") as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    # 读取提示文件 prompt.json\n",
    "    with open(osp.join(base_dir, \"prompt.json\"), \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "        task_description = prompt[\"task_description\"]  # 提取任务描述\n",
    "\n",
    "    # 遍历每个想法，检查其创新性\n",
    "    for idx, idea in enumerate(ideas):\n",
    "        if \"novel\" in idea:\n",
    "            print(f\"Skipping idea {idx}, already checked.\")  # 如果已经检查过，跳过\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nChecking novelty of idea {idx}: {idea['Name']}\")\n",
    "\n",
    "        novel = False  # 初始化标志，表示是否为创新\n",
    "        msg_history = []  # 消息历史，用于跟踪对话\n",
    "        papers_str = \"\"  # 存储找到的论文信息\n",
    "\n",
    "        for j in range(max_num_iterations):\n",
    "            try:\n",
    "                # 调用 LLM 获取响应\n",
    "                text, msg_history = get_response_from_llm(\n",
    "                    novelty_prompt.format(\n",
    "                        current_round=j + 1,  # 当前轮次\n",
    "                        num_rounds=max_num_iterations,  # 总轮次\n",
    "                        idea=idea,  # 当前想法\n",
    "                        last_query_results=papers_str,  # 上一轮查询结果\n",
    "                    ),\n",
    "                    client=client,\n",
    "                    model=model,\n",
    "                    system_message=novelty_system_msg.format(\n",
    "                        num_rounds=max_num_iterations,  # 总轮次\n",
    "                        task_description=task_description,  # 任务描述\n",
    "                        code=code,  # 实验代码\n",
    "                    ),\n",
    "                    msg_history=msg_history,  # 消息历史\n",
    "                )\n",
    "                \n",
    "                # 检查响应中是否包含“novel”或“not novel”的决策\n",
    "                if \"decision made: novel\" in text.lower():\n",
    "                    print(\"Decision made: novel after round\", j)\n",
    "                    novel = True\n",
    "                    break\n",
    "                if \"decision made: not novel\" in text.lower():\n",
    "                    print(\"Decision made: not novel after round\", j)\n",
    "                    break\n",
    "\n",
    "                # 解析输出中的 JSON 数据\n",
    "                json_output = extract_json_between_markers(text)\n",
    "                assert json_output is not None, \"Failed to extract JSON from LLM output\"\n",
    "\n",
    "                # 搜索相关论文\n",
    "                query = json_output[\"Query\"]  # 从 JSON 中获取查询字符串\n",
    "                papers = search_for_papers(query, result_limit=10)  # 查询论文\n",
    "                if papers is None:\n",
    "                    papers_str = \"No papers found.\"\n",
    "\n",
    "                # 将找到的论文格式化为字符串\n",
    "                paper_strings = []\n",
    "                for i, paper in enumerate(papers):\n",
    "                    paper_strings.append(\n",
    "                        \"\"\"{i}: {title}. {authors}. {venue}, {year}.\\nNumber of citations: {cites}\\nAbstract: {abstract}\"\"\".format(\n",
    "                            i=i,\n",
    "                            title=paper[\"title\"],\n",
    "                            authors=paper[\"authors\"],\n",
    "                            venue=paper[\"venue\"],\n",
    "                            year=paper[\"year\"],\n",
    "                            cites=paper[\"citationCount\"],\n",
    "                            abstract=paper[\"abstract\"],\n",
    "                        )\n",
    "                    )\n",
    "                papers_str = \"\\n\\n\".join(paper_strings)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        idea[\"novel\"] = novel  # 将创新性结果存入想法字典中\n",
    "\n",
    "    # 将结果保存到 JSON 文件中\n",
    "    results_file = osp.join(base_dir, \"ideas.json\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(ideas, f, indent=4)\n",
    "\n",
    "    return ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./template_data/grokking/\"\n",
    "client = openai.OpenAI()\n",
    "model = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking novelty of idea 0: batch_size_grokking\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 1: attention_patterns_grokking\n",
      "Error: 'NoneType' object is not iterable\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 2: optimization_strategy_grokking\n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=grokking+neural+networks+optimization+learning+rate&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 3: representation_dynamics_grokking\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 4: architecture_complexity_grokking\n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=Transformer+architecture+grokking&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 5: noise_impact_grokking\n",
      "Decision made: novel after round 1\n",
      "\n",
      "Checking novelty of idea 6: weight_sparsity_grokking\n",
      "Decision made: not novel after round 2\n",
      "\n",
      "Checking novelty of idea 7: data_augmentation_grokking\n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=data+augmentation+grokking+generalization+neural+networks&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 8: curriculum_learning_grokking\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 9: representation_phase_grokking\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 10: initialization_grokking\n",
      "Error: 'NoneType' object is not iterable\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 11: interpretability_grokking\n",
      "Decision made: novel after round 1\n",
      "\n",
      "Checking novelty of idea 12: layer_interaction_grokking\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 13: negative_examples_grokking\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 14: latent_space_traversal\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 15: interpolation_dynamics_grokking\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 16: sequence_length_grokking\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 17: data_complexity_grokking\n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=grokking+and+problem+complexity+in+neural+networks&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Decision made: novel after round 3\n",
      "\n",
      "Checking novelty of idea 18: symmetry_enhancement_grokking\n",
      "Error: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=symmetry+data+augmentation+neural+networks&limit=10&fields=title%2Cauthors%2Cvenue%2Cyear%2Cabstract%2CcitationStyles%2CcitationCount (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "Decision made: novel after round 2\n",
      "\n",
      "Checking novelty of idea 19: meta_learning_grokking\n",
      "Decision made: novel after round 1\n",
      "\n",
      "Checking novelty of idea 20: invariant_transformations_grokking\n",
      "Decision made: novel after round 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Name': 'batch_size_grokking',\n",
       "  'Title': 'Batch Size Grokking: Assessing the impact of the training batchsize on the grokking phenomenon',\n",
       "  'Experiment': 'Modify the experiments to dynamically adjust the batch size during training, starting with a small batch size and gradually increasing it. This could potentially lead to faster generalization on the validation set.',\n",
       "  'Interestingness': 6,\n",
       "  'Feasibility': 4,\n",
       "  'Novelty': 4,\n",
       "  'novel': True},\n",
       " {'Name': 'attention_patterns_grokking',\n",
       "  'Title': 'Attention Patterns During Grokking: Analyzing the Evolution of Attention in Transformer Models',\n",
       "  'Experiment': \"Implement hooks in the self-attention layers of the `DecoderBlock` class to log attention scores at regular intervals during training (e.g., every 1000 steps). Use these logs to visualize and analyze changes in attention distributions over time. Apply clustering techniques or statistical methods to compare attention patterns before, during, and after grokking. This analysis should aim to identify any shifts in focus that correlate with the model's transition to perfect generalization.\",\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'optimization_strategy_grokking',\n",
       "  'Title': 'Optimization Strategy and Grokking: Exploring the Role of Optimizers and Learning Rates',\n",
       "  'Experiment': 'Implement and compare the effects of different optimizers (e.g., SGD, RMSprop, Adam) and learning rate schedules (e.g., step decay, cosine annealing) on the grokking phenomenon. Modify the run function to include these variations and log the time it takes for models to achieve perfect generalization. Analyze how different strategies affect the convergence speed and stability of grokking across datasets.',\n",
       "  'Interestingness': 7,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 6,\n",
       "  'novel': True},\n",
       " {'Name': 'representation_dynamics_grokking',\n",
       "  'Title': 'Representation Dynamics and Grokking: Analyzing the Evolution of Feature Representations in Transformer Models',\n",
       "  'Experiment': 'Implement hooks in the Transformer model to extract and log hidden activations from the first and last layers at regular intervals, such as every 1000 training steps. Use dimensionality reduction techniques like PCA or t-SNE to visualize these activations, focusing on changes in clustering and separability across different classes. This analysis should be conducted before, during, and after grokking, aiming to identify representation patterns associated with the transition to perfect generalization. Visualizations will be used for both qualitative insights and quantitative measures like cluster compactness.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'architecture_complexity_grokking',\n",
       "  'Title': 'Architecture Complexity and Grokking: Exploring the Role of Model Depth and Width',\n",
       "  'Experiment': 'Implement experiments to vary the number of layers and attention heads in the Transformer model. Modify the initialization parameters in the Transformer class to allow for easy adjustment of these architectural features. Run systematic experiments with different combinations of depth and width, measuring the time it takes for models to achieve perfect generalization and analyzing the relationship between architectural complexity and grokking. Use the final training and validation metrics to assess how these changes affect performance and generalization.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'noise_impact_grokking',\n",
       "  'Title': 'Noise Impact on Grokking: Assessing the Role of Noise in Neural Network Generalization',\n",
       "  'Experiment': 'Incorporate noise injection into the training process by modifying the train function. Implement options to add Gaussian noise to input data or hidden layers, with adjustable noise levels. Introduce noise at specific intervals during training to observe its impact at different training stages. Conduct experiments with varying noise levels to measure their effect on the grokking timeline and final generalization performance. Use metrics such as the number of training steps to achieve 99% validation accuracy and final validation loss to assess the impact.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'weight_sparsity_grokking',\n",
       "  'Title': 'Weight Sparsity and Grokking: Exploring the Impact of Pruning on Generalization',\n",
       "  'Experiment': 'Implement a pruning mechanism within the training loop to periodically remove a fraction of the least significant weights based on magnitude. Modify the train function to include pruning steps after certain training intervals. Analyze the number of training steps required to reach 99% validation accuracy and final validation performance across different pruning strategies. This will help assess how pruning impacts the grokking timeline and generalization ability.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 8,\n",
       "  'novel': False},\n",
       " {'Name': 'data_augmentation_grokking',\n",
       "  'Title': 'Data Augmentation and Grokking: Exploring the Impact of Input Variability on Generalization',\n",
       "  'Experiment': 'Implement data augmentation techniques specific to each dataset type: for ModSum and ModSubtract, apply operand swapping (e.g., a+b = b+a); for ModDivision, introduce multiplicative inverses; for Permutation, use input permutation. Modify the data loading pipeline in the `GroupDataset` class to include these augmentations during training. Ensure that the validation data remains unchanged for clear comparison. Conduct experiments to measure the effect of data augmentation on the number of training steps required to achieve 99% validation accuracy and on the final validation loss, comparing these metrics with baseline results without augmentation.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 7,\n",
       "  'novel': True},\n",
       " {'Name': 'curriculum_learning_grokking',\n",
       "  'Title': 'Curriculum Learning and Grokking: The Impact of Progressive Training Difficulty',\n",
       "  'Experiment': 'Modify the data loading pipeline to implement a curriculum learning strategy. Create a mechanism to rank the difficulty of training examples, starting with simpler ones and gradually progressing to more complex ones. This can involve starting with smaller numbers for mathematical operations and slowly increasing their range as training progresses. Implement this progression in the `GroupDataset` class, and adjust the training loop to handle the dynamic data selection. Measure the time taken to achieve 99% validation accuracy and compare it to baseline results to determine the impact of curriculum learning on grokking.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 9,\n",
       "  'novel': True},\n",
       " {'Name': 'representation_phase_grokking',\n",
       "  'Title': 'Representation Phase Transition and Grokking: Analyzing Hidden State Dynamics',\n",
       "  'Experiment': 'Modify the Transformer model to include hooks that capture activations from the beginning, middle, and end layers at predefined training intervals (e.g., every 1000 steps). Use PCA or t-SNE for dimensionality reduction of captured activations, and visualize them to observe structural changes over time. Implement scripts to automate the logging and visualization process, focusing on identifying distinctive patterns or transformations in the hidden states as the model transitions towards perfect generalization.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'initialization_grokking',\n",
       "  'Title': 'Initialization and Grokking: Exploring the Influence of Initial Weights on Learning Dynamics',\n",
       "  'Experiment': 'Modify the Transformer model initialization in the __init__ method to allow for different initialization strategies such as Xavier, He, and random Gaussian with varying variances. Conduct experiments to observe how these different initializations affect the number of steps required to achieve perfect generalization and the learning trajectory. Use metrics like training speed, final validation accuracy, and loss curves to assess the impact. Compare results against a baseline with default initialization.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'interpretability_grokking',\n",
       "  'Title': 'Interpretability and Grokking: Analyzing Attention Weight Dynamics During Generalization',\n",
       "  'Experiment': 'Implement functionality to extract and log attention weights from the Transformer model at regular intervals, such as every 1000 steps. Modify the training loop to compute these weights and analyze how their distribution changes over time, particularly during the transition from overfitting to generalization. Use visualizations to illustrate shifts in attention focus and compare patterns across different datasets, aiming to identify common attention features that signify grokking.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'layer_interaction_grokking',\n",
       "  'Title': 'Layer Interaction and Grokking: Investigating Inter-Layer Communication in Neural Networks',\n",
       "  'Experiment': 'Implement hooks to log layer outputs at regular intervals and analyze correlations between these layer outputs during training. Focus on determining how different layers influence the learning process and contribute to grokking. Measure changes in these correlations as the model transitions from overfitting to generalization. Evaluate how this inter-layer communication correlates with performance metrics such as validation accuracy and loss.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'negative_examples_grokking',\n",
       "  'Title': 'Negative Examples and Grokking: Exploring the Role of Incorrect Data in Generalization',\n",
       "  'Experiment': 'Modify the fetch_train_example method in the dataset classes to introduce a configurable fraction of negative examples, where the model is presented with incorrect outputs for given inputs. Implement a mechanism to control the ratio of negative to positive examples during training. Conduct experiments with varying ratios, measuring their impact on the number of training steps to achieve 99% validation accuracy and the final validation loss. Compare these metrics to baseline results to assess how negative examples influence grokking.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'latent_space_traversal',\n",
       "  'Title': 'Latent Space Traversal and Grokking: Exploring Parameter Perturbations and Generalization',\n",
       "  'Experiment': \"Modify the train function to periodically apply small, controlled Gaussian noise to the model's parameters or interpolate between parameter states during training, ensuring perturbations are subtle. Implement visualization techniques to track changes in the latent space over time, and analyze their effect on the model's path towards perfect generalization by monitoring validation accuracy and loss. Compare with baseline runs to identify perturbation patterns that stabilize or accelerate grokking.\",\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 9,\n",
       "  'novel': True},\n",
       " {'Name': 'interpolation_dynamics_grokking',\n",
       "  'Title': 'Interpolation Dynamics and Grokking: Analyzing Hidden Representation Evolution',\n",
       "  'Experiment': 'Modify the Transformer model to include hooks that capture hidden activations at regular intervals, such as every 1000 steps. Implement a mechanism to compute cosine similarities between hidden layer activations for a diverse and representative set of input pairs, selected through clustering or stratified sampling. Log these metrics over time and analyze trends that emerge during the transition from overfitting to grokking. Quantify shifts in representation interpolation dynamics using statistical measures and visualize these trends to provide clear insights. Discuss potential implications for enhancing model training and generalization strategies.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 9,\n",
       "  'novel': True},\n",
       " {'Name': 'sequence_length_grokking',\n",
       "  'Title': 'Sequence Length and Grokking: Investigating the Impact of Input Length on Learning Dynamics',\n",
       "  'Experiment': 'Modify the data encoding process to generate input sequences of varying lengths by altering the form_equation method in AbstractDataset. Adjust the Transformer model to accept variable-length sequences by modifying the position_embeddings and related components. Conduct experiments with different sequence lengths, measuring the number of training steps required to achieve 99% validation accuracy and final validation loss. Compare these metrics across sequence lengths to assess their influence on grokking.',\n",
       "  'Interestingness': 8,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 8,\n",
       "  'novel': True},\n",
       " {'Name': 'data_complexity_grokking',\n",
       "  'Title': 'Data Complexity and Grokking: Investigating the Role of Problem Intricacy in Learning Dynamics',\n",
       "  'Experiment': 'Implement a function to measure problem size characteristics, such as the size of modulo and permutation length, for each dataset type. Modify the run function to log these complexity scores alongside training metrics. Conduct experiments across datasets and correlate problem size with grokking characteristics, such as training steps to achieve 99% validation accuracy and final validation loss. Analyze whether larger or more intricate problems correlate with delayed grokking. Use statistical measures to validate findings and provide insights on the relationship between problem size and model generalization.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 9,\n",
       "  'novel': True},\n",
       " {'Name': 'symmetry_enhancement_grokking',\n",
       "  'Title': 'Symmetry Enhancement and Grokking: Leveraging Invariances for Improved Generalization',\n",
       "  'Experiment': \"Focus initially on the ModSumDataset and modify the loss function to include a symmetry enforcement term. This term will be computed by evaluating the model's output on both original and operand-swapped inputs, and penalizing dissimilar outputs. Update the train function to incorporate this new loss component, and monitor its impact on the grokking timeline by comparing the number of steps required to achieve 99% validation accuracy and final validation performance against baseline results without symmetry enhancement.\",\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 8,\n",
       "  'Novelty': 9,\n",
       "  'novel': True},\n",
       " {'Name': 'meta_learning_grokking',\n",
       "  'Title': 'Meta-Learning and Grokking: Leveraging Learning-to-Learn for Improved Generalization',\n",
       "  'Experiment': 'Implement a meta-learning loop by modifying the run function. Divide training into several phases, each using a different permutation of the dataset. Between phases, save model parameters and reinitialize the model to continue training with these parameters. Log metrics on the steps required to reach 99% validation accuracy for each phase, and compare improvements across phases to assess meta-learning impact.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 9,\n",
       "  'novel': True},\n",
       " {'Name': 'invariant_transformations_grokking',\n",
       "  'Title': 'Invariant Transformations and Grokking: Exploring Semantically Stable Data Augmentation',\n",
       "  'Experiment': 'Implement a transformation generator that creates rule-based transformations specific to each dataset type, such as operand swapping or scaling for arithmetic operations that maintain the result modulo p. Modify the data pipeline to apply these transformations during training, ensuring the validation set remains unchanged for consistency in evaluation. Run experiments to measure the impact on the number of training steps required to achieve 99% validation accuracy and final validation loss, comparing these results to baseline runs without transformations.',\n",
       "  'Interestingness': 9,\n",
       "  'Feasibility': 7,\n",
       "  'Novelty': 8,\n",
       "  'novel': True}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(osp.join(BASE_DIR, \"ideas.json\"), \"r\") as f:\n",
    "    ideas = json.load(f)\n",
    "check_idea_novelty(ideas, BASE_DIR, client, model, max_num_iterations=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
